{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual of DATE (Dual-Attentive-Tree-aware-Embedding) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Date: 2020. 5. 27.\n",
    "* Written by Yeon Soo Choi, Technical Officer, Research Unit, WCO\n",
    "* Revised by Sundong Kim, Institute for Basic Science\n",
    "* Original paper\n",
    "    * Title: DATE: Dual Attentive Tree-aware Embedding for Customs Frauds Detection\n",
    "    * Authors\n",
    "        - IBS: Sundong Kim*, Karandeep Singh, Meeyoung Cha\n",
    "        - NCKU: Yu-Che Tsaiâˆ—, Cheng-Te Li\n",
    "        - WCO: Yeon Soo Choi\n",
    "        - NCS: Etim Ibok\n",
    "    * Source: https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding\n",
    "        - All the python codes are included in this one notebook.\n",
    "* Data: Synthetic import data (xxx at transaction-item level)\n",
    "\n",
    "\n",
    "## Summary (in non-technical terms)\n",
    "\n",
    "**DATE (Dual-Attentive-Tree-aware-Embedding)** is a neural network model to detect undervalued imports.  \n",
    "\n",
    "Imagine that you **(\"neural networks\")** are the head of a Customs targeting centre composed of **100 risk analysts (\"decision trees\")**. For a given import, you task the analysts with reporting **the probability of undervaluation** and **the estimate of additional revenue from the inspection (\"dual-task\")**.  \n",
    "\n",
    "How would you put 100 different reports together in making your final decision?\n",
    "Simply averaging their predictions may neglect some valuable information hidden in 100 reports. The DATE model help you keep all the information while paying more **ATTENTION** to specific pieces. Firstly, if there are a majority group of reports significantly similar to each other, you may pay more **ATTENTION** to those reports. Secondly, if you have analysts specialized in the specific HS code and importer of the given import, you may pay more **ATTENTION** to their reports. In the end, you make your final decision based on all the reports, however, in proportion to the amount of attention you paid to respective reports.\n",
    "\n",
    "## Summary (in technical terms)\n",
    "\n",
    "Now, lets take an overview of the model with some technical terms. For a given import, the DATE model works in the following steps;\n",
    "\n",
    "* **XGBoost**: The model passes the import into a XGBoost model which constructs multiple(eg. 100) decision trees. \n",
    "* **Embedding**: Each tree's decision (decision path, leaf-id) is transformed into a set of numbers to be fed into neural networks. \n",
    "* **Multi-head Self-attention**\n",
    "    - Self-attention: The numeric value (importance, weight) of each leaf-id is adjusted based on its correlation/interaction with other leaf-ids.\n",
    "    - Multi-head: Self-iteration is repeated in multiple times to achieve its robustness.\n",
    "* **Attention**: The numeric values (importance, weight) of each leaf-id is re-adjusted based on its correlation/interaction with the given importer-id and item-id(HScode).\n",
    "\n",
    "## OUTLINE\n",
    "* [Part 1. Preprocess data](#part1)\n",
    "* [Part 2. XGBoost model](#part2)\n",
    "* [Part 3. DATE (XGBoost + Neural Networks + Attention)](#part3)\n",
    "* [Part 4. Evaluation](#part4)\n",
    "* [Part 5. Practice of functions](#part5)\n",
    "* [Part 6. XGBoost + Logistic Regression model](#part6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preprocess <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "pd.set_option('display.max_columns',100)\n",
    "from collections import defaultdict\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/synthetic-imports-declarations.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sgd.id</th>\n",
       "      <th>sgd.date</th>\n",
       "      <th>importer.id</th>\n",
       "      <th>declarant.id</th>\n",
       "      <th>country</th>\n",
       "      <th>office.id</th>\n",
       "      <th>tariff.code</th>\n",
       "      <th>quantity</th>\n",
       "      <th>gross.weight</th>\n",
       "      <th>fob.value</th>\n",
       "      <th>cif.value</th>\n",
       "      <th>total.taxes</th>\n",
       "      <th>illicit</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD1</td>\n",
       "      <td>13-01-02</td>\n",
       "      <td>IMP826164</td>\n",
       "      <td>DEC3207</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>OFFICE92</td>\n",
       "      <td>8703241128</td>\n",
       "      <td>1581</td>\n",
       "      <td>26494</td>\n",
       "      <td>2390</td>\n",
       "      <td>2809</td>\n",
       "      <td>647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD2</td>\n",
       "      <td>13-01-02</td>\n",
       "      <td>IMP837219</td>\n",
       "      <td>DEC1525</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>OFFICE51</td>\n",
       "      <td>8703232926</td>\n",
       "      <td>1</td>\n",
       "      <td>3910</td>\n",
       "      <td>204098</td>\n",
       "      <td>266140</td>\n",
       "      <td>3262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD3</td>\n",
       "      <td>13-01-02</td>\n",
       "      <td>IMP117406</td>\n",
       "      <td>DEC4146</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>OFFICE59</td>\n",
       "      <td>8517180000</td>\n",
       "      <td>1</td>\n",
       "      <td>699231</td>\n",
       "      <td>302275</td>\n",
       "      <td>302275</td>\n",
       "      <td>5612</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD4</td>\n",
       "      <td>13-01-02</td>\n",
       "      <td>IMP435108</td>\n",
       "      <td>DEC4242</td>\n",
       "      <td>CNTRY376</td>\n",
       "      <td>OFFICE51</td>\n",
       "      <td>8703222900</td>\n",
       "      <td>1288</td>\n",
       "      <td>22958</td>\n",
       "      <td>3019</td>\n",
       "      <td>4160</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD5</td>\n",
       "      <td>13-01-02</td>\n",
       "      <td>IMP717900</td>\n",
       "      <td>DEC6324</td>\n",
       "      <td>CNTRY454</td>\n",
       "      <td>OFFICE92</td>\n",
       "      <td>8545200000</td>\n",
       "      <td>42</td>\n",
       "      <td>21248</td>\n",
       "      <td>156348</td>\n",
       "      <td>239549</td>\n",
       "      <td>397</td>\n",
       "      <td>1</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sgd.id  sgd.date importer.id declarant.id   country office.id  tariff.code  \\\n",
       "0   SGD1  13-01-02   IMP826164      DEC3207  CNTRY680  OFFICE92   8703241128   \n",
       "1   SGD2  13-01-02   IMP837219      DEC1525  CNTRY680  OFFICE51   8703232926   \n",
       "2   SGD3  13-01-02   IMP117406      DEC4146  CNTRY680  OFFICE59   8517180000   \n",
       "3   SGD4  13-01-02   IMP435108      DEC4242  CNTRY376  OFFICE51   8703222900   \n",
       "4   SGD5  13-01-02   IMP717900      DEC6324  CNTRY454  OFFICE92   8545200000   \n",
       "\n",
       "   quantity  gross.weight  fob.value  cif.value  total.taxes  illicit  revenue  \n",
       "0      1581         26494       2390       2809          647        0        0  \n",
       "1         1          3910     204098     266140         3262        0        0  \n",
       "2         1        699231     302275     302275         5612        0        0  \n",
       "3      1288         22958       3019       4160          514        0        0  \n",
       "4        42         21248     156348     239549          397        1      980  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to use\n",
    "column_to_use = ['sgd.date','office.id','importer.id', \n",
    "                 'declarant.id','tariff.code','country',\n",
    "                 'cif.value','quantity','gross.weight',\n",
    "                 'total.taxes','revenue','illicit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "df = df[column_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('sgd.date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 0 to 99999\n",
      "Data columns (total 12 columns):\n",
      "sgd.date        100000 non-null object\n",
      "office.id       100000 non-null object\n",
      "importer.id     100000 non-null object\n",
      "declarant.id    100000 non-null object\n",
      "tariff.code     100000 non-null int64\n",
      "country         100000 non-null object\n",
      "cif.value       100000 non-null int64\n",
      "quantity        100000 non-null int64\n",
      "gross.weight    100000 non-null int64\n",
      "total.taxes     100000 non-null int64\n",
      "revenue         100000 non-null int64\n",
      "illicit         100000 non-null int64\n",
      "dtypes: int64(7), object(5)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Define functions to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. \"merge_attributes\" function [(link to practice)](#practice1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"merge_attributes\" fuction is to create a new categorical variable by combining multiple existing categorical variables into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_attributes(df: pd.DataFrame, *args: str) -> None: \n",
    "    # Note: \"*args\" represents multiple arguments, i.e. multiple variable names could come. \n",
    "    # Note: \"-> None\" represents that this function returns None (i.e. type annotation)\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype *args: strings (attribute names that want to be combined)\n",
    "    \"\"\"\n",
    "    # To set data type of each argument as string\n",
    "    iterables = [df[arg].astype(str) for arg in args] \n",
    "    # To name the newly combined variable/column\n",
    "    columnName = '&'.join([*args]) \n",
    "    # To create a column for the combined variable\n",
    "    fs = [''.join([v for v in var]) for var in zip(*iterables)] # \"*\" represents \"unzip\"\n",
    "    df.loc[:, columnName] = fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. \"preprocess\" function [(link to practice)](#practice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fuction is to;\n",
    "* generate additional features such as unitprice, weight-unitprice and effective tariff rate;\n",
    "* merge some attributes, using the above \"merge_attributes\" function; and \n",
    "* generate date-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Note: \"-> pd.DataFrame\" represents that this function returns a dataframe.\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    rtype df: dataframe\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['total.taxes']) # Remove rows which does not have these values.\n",
    "    df.loc[:, 'Unitprice'] = df['cif.value']/df['quantity']\n",
    "    df.loc[:, 'WUnitprice'] = df['cif.value']/df['gross.weight']\n",
    "    df.loc[:, 'TaxRatio'] = df['total.taxes'] / df['cif.value']\n",
    "    df.loc[:, 'TaxUnitquantity'] = df['total.taxes'] / df['quantity']\n",
    "    df.loc[:, 'HS6'] = df['tariff.code'].apply(lambda x: int(x // 10000))\n",
    "    df.loc[:, 'HS4'] = df['HS6'].apply(lambda x: int(x // 100))\n",
    "    df.loc[:, 'HS2'] = df['HS4'].apply(lambda x: int(x // 100))\n",
    "    \n",
    "    # Made a general function \"merge_attributes\" for supporting any combination    \n",
    "    merge_attributes(df, 'HS6','country')\n",
    "    merge_attributes(df, 'office.id','importer.id')\n",
    "    merge_attributes(df, 'office.id','HS6')\n",
    "    merge_attributes(df, 'office.id','country')\n",
    "    \n",
    "    # another way of combining features\n",
    "    #df.loc[:, 'HS6.country'] = [str(i)+'&'+j for i, j in zip(df['HS6'], df['country'])]\n",
    "    \n",
    "    \n",
    "    # Day of Year of sgd.date\n",
    "    tmp2 = {}\n",
    "    for date in set(df['sgd.date']):\n",
    "        tmp2[date] = dt.strptime(date, '%y-%m-%d') \n",
    "    tmp_day = {}\n",
    "    tmp_week = {}\n",
    "    tmp_month = {}\n",
    "    yearStart = dt(tmp2[date].date().year, 1, 1)\n",
    "    for item in tmp2:\n",
    "        tmp_day[item] = (tmp2[item] - yearStart).days\n",
    "        tmp_week[item] = int(tmp_day[item] / 7)\n",
    "        tmp_month[item] = int(tmp_day[item] / 30)\n",
    "        \n",
    "    df.loc[:, 'SGD.DayofYear'] = df['sgd.date'].apply(lambda x: tmp_day[x])\n",
    "    df.loc[:, 'SGD.WeekofYear'] = df['sgd.date'].apply(lambda x: tmp_week[x])\n",
    "    df.loc[:, 'SGD.MonthofYear'] = df['sgd.date'].apply(lambda x: tmp_month[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. \"find_risk_profile\" function [(link to practice)](#practice3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to identify/calculate risk-profiling indicators of the features;\n",
    "* option 1 (topk): Lists of top n high-risk categories in the features\n",
    "* option 2 (ratio): illicit ratio of categories in the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk_profile(df: pd.DataFrame, \n",
    "                      feature: str, \n",
    "                      topk_ratio: float, \n",
    "                      adj: float, \n",
    "                      option: str) -> list or dict:\n",
    "    \"\"\"\n",
    "    dtype feature: str\n",
    "    dtype topk_ratio: float (range: 0-1)\n",
    "    dtype adj: float (to modify the mean)\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: list(option='topk') or dict(option='ratio')\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        # Group data by a specified feature(vairable)\n",
    "        total_cnt = df.groupby([feature])['illicit']\n",
    "        # Set the number of entities to be included a black list.\n",
    "        nrisky_profile = int(topk_ratio*len(total_cnt))+1\n",
    "        # For each entity, calculate 'total number of frauds' divided by 'total number of imports' \n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return list(adj_prob_illicit.sort_values(ascending=False).head(nrisky_profile).index)\n",
    "    \n",
    "    # Illicit-ratio encoding (Mean target encoding)\n",
    "    # Refer: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html\n",
    "    # Refer: https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0\n",
    "    elif option == 'ratio':\n",
    "        # For target encoding, we just use 70% of train data to avoid overfitting (otherwise, test AUC drops significantly)\n",
    "        total_cnt = df.sample(frac=0.7).groupby([feature])['illicit']\n",
    "        # prob_illicit = total_cnt.mean()  # Simple mean\n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return adj_prob_illicit.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. \"tag_risky_profiles\" function [(link to practice)](#practice4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to generate risk-profiling tags of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_risky_profiles(df: pd.DataFrame, \n",
    "                       profile: str, \n",
    "                       profiles: list or dict, \n",
    "                       option: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype profile: str\n",
    "    dtype profiles: list(option='topk') or dictionary(option='ratio')\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: dataframe\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        d = defaultdict(int) # return 0 for not-defined keys\n",
    "        for id in profiles:\n",
    "            d[id] = 1\n",
    "    #     print(list(islice(d.items(), 10)))  # For debugging\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: d[x])\n",
    "    \n",
    "    # Illicit-ratio encoding\n",
    "    elif option == 'ratio':\n",
    "        overall_ratio_train = np.mean(train.illicit) # When scripting, saving it as a class variable is clearer.\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: profiles.get(x,overall_ratio_train))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.4. Preprocess data with pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset settings\n",
    "data_length = df.shape[0]\n",
    "train_ratio = 0.6\n",
    "valid_ratio = 0.8\n",
    "train_length = int(data_length*train_ratio)\n",
    "valid_length = int(data_length*valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way of splitting train/valid/test set\n",
    "df=df.sort_values('sgd.date')\n",
    "train = df.iloc[:train_length,:]\n",
    "valid = df.iloc[train_length:valid_length,:]\n",
    "test = df.iloc[valid_length:,:]\n",
    "\n",
    "## Another way of splitting - explicitly split by time\n",
    "#train = df[df[\"sgd.date\"] < \"2013-11-01\"]\n",
    "#valid = df[(df[\"sgd.date\"] >= \"2013-11-01\") & (df[\"SGD.DATE\"] < \"2013-11-01\")]\n",
    "#test = df[df[\"sgd.date\"] >= \"2013-12-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 12), (20000, 12), (20000, 12))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sgd.date', 'office.id', 'importer.id', 'declarant.id', 'tariff.code',\n",
       "       'country', 'cif.value', 'quantity', 'gross.weight', 'total.taxes',\n",
       "       'revenue', 'illicit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label data\n",
    "train_reg_label = train['revenue'].values\n",
    "valid_reg_label = valid['revenue'].values\n",
    "test_reg_label = test['revenue'].values\n",
    "train_cls_label = train[\"illicit\"].values\n",
    "valid_cls_label = valid[\"illicit\"].values\n",
    "test_cls_label = test[\"illicit\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "train = preprocess(train)\n",
    "valid = preprocess(valid)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sgd.date', 'office.id', 'importer.id', 'declarant.id', 'tariff.code',\n",
       "       'country', 'cif.value', 'quantity', 'gross.weight', 'total.taxes',\n",
       "       'revenue', 'illicit', 'Unitprice', 'WUnitprice', 'TaxRatio',\n",
       "       'TaxUnitquantity', 'HS6', 'HS4', 'HS2', 'HS6&country',\n",
       "       'office.id&importer.id', 'office.id&HS6', 'office.id&country',\n",
       "       'SGD.DayofYear', 'SGD.WeekofYear', 'SGD.MonthofYear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few more risky profiles\n",
    "risk_profiles = {}\n",
    "profile_candidates = ['importer.id', 'declarant.id', 'tariff.code', 'quantity', 'HS6', 'HS4', 'HS2', 'office.id'] + [col for col in train.columns if '&' in col]\n",
    "\n",
    "for profile in profile_candidates:\n",
    "    option = 'topk'\n",
    "    risk_profiles[profile] = find_risk_profile(train, profile, 0.1, 10, option=option)\n",
    "    train = tag_risky_profiles(train, profile, risk_profiles[profile], option=option)\n",
    "    valid = tag_risky_profiles(valid, profile, risk_profiles[profile], option=option)\n",
    "    test = tag_risky_profiles(test, profile, risk_profiles[profile], option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sgd.date', 'office.id', 'importer.id', 'declarant.id', 'tariff.code',\n",
       "       'country', 'cif.value', 'quantity', 'gross.weight', 'total.taxes',\n",
       "       'revenue', 'illicit', 'Unitprice', 'WUnitprice', 'TaxRatio',\n",
       "       'TaxUnitquantity', 'HS6', 'HS4', 'HS2', 'HS6&country',\n",
       "       'office.id&importer.id', 'office.id&HS6', 'office.id&country',\n",
       "       'SGD.DayofYear', 'SGD.WeekofYear', 'SGD.MonthofYear',\n",
       "       'RiskH.importer.id', 'RiskH.declarant.id', 'RiskH.tariff.code',\n",
       "       'RiskH.quantity', 'RiskH.HS6', 'RiskH.HS4', 'RiskH.HS2',\n",
       "       'RiskH.office.id', 'RiskH.HS6&country', 'RiskH.office.id&importer.id',\n",
       "       'RiskH.office.id&HS6', 'RiskH.office.id&country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use in a classifier\n",
    "column_to_use = ['cif.value', 'total.taxes', 'gross.weight', 'quantity', 'Unitprice', 'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'tariff.code', 'HS6', 'HS4', 'HS2', 'SGD.DayofYear', 'SGD.WeekofYear', 'SGD.MonthofYear'] + [col for col in train.columns if 'RiskH' in col] \n",
    "\n",
    "# Extract only numeric values from data to be fed into models\n",
    "X_train = train[column_to_use].values\n",
    "X_valid = valid[column_to_use].values\n",
    "X_test = test[column_to_use].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute nan\n",
    "X_train = np.nan_to_num(X_train, 0)\n",
    "X_valid = np.nan_to_num(X_valid, 0)\n",
    "X_test = np.nan_to_num(X_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data size...\n",
      "(60000, 27) (60000,) (60000,)\n",
      "(20000, 27) (20000,) (20000,)\n",
      "(20000, 27) (20000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "# make sure the data size are correct\n",
    "print(\"Checking data size...\")\n",
    "print(X_train.shape, train_cls_label.shape, train_reg_label.shape)\n",
    "print(X_valid.shape, valid_cls_label.shape, valid_reg_label.shape)\n",
    "print(X_test.shape, test_cls_label.shape, test_reg_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Save all the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all data in a dictionary\n",
    "all_data = {\"raw\":{\"train\":train,\"valid\":valid,\"test\":test},\n",
    " \"xgboost_data\":{\"train_x\":X_train,\"train_y\":train_cls_label,\\\n",
    "                 \"valid_x\":X_valid,\"valid_y\":valid_cls_label,\\\n",
    "                 \"test_x\":X_test,\"test_y\":test_cls_label},\n",
    " \"revenue\":{\"train\":train_reg_label,\"valid\":valid_reg_label,\"test\":test_reg_label}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking label distribution\n",
      "Training: 0.08246585722275343\n",
      "Validation: 0.0803219359369092\n",
      "Testing: 0.08230964879051897\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Checking label distribution\")\n",
    "cnt = Counter(train_cls_label)\n",
    "print(\"Training:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(valid_cls_label)\n",
    "print(\"Validation:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(test_cls_label)\n",
    "print(\"Testing:\",cnt[1]/cnt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a variable to a file\n",
    "# reference for pickle: https://www.datacamp.com/community/tutorials/pickle-python-tutorial\n",
    "file = open('./processed_data.pickle', 'wb')\n",
    "pickle.dump(all_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. XGBoost model <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import copy\n",
    "import os \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load the preprocessed data in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['raw', 'xgboost_data', 'revenue'])\n",
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data\n",
    "with open(\"./processed_data.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)\n",
    "print(processed_data.keys())\n",
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data \n",
    "train = processed_data[\"raw\"][\"train\"]\n",
    "valid = processed_data[\"raw\"][\"valid\"]\n",
    "test = processed_data[\"raw\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split labels into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue data for regression target \n",
    "revenue_train = processed_data[\"revenue\"][\"train\"]\n",
    "revenue_valid = processed_data[\"revenue\"][\"valid\"]\n",
    "revenue_test = processed_data[\"revenue\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take logged values of revenue, and normalize them. --> getting more densed distribution/minimizing outliers' impacts.  \n",
    "As we assume no information on valid-data and test-data, they are normalized with the information of train-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize revenue by f(x) = log(x+1)/max(xi)\n",
    "norm_revenue_train = np.log(revenue_train+1)\n",
    "norm_revenue_valid = np.log(revenue_valid+1)\n",
    "norm_revenue_test = np.log(revenue_test+1) \n",
    "global_max = max(norm_revenue_train) \n",
    "norm_revenue_train = norm_revenue_train/global_max\n",
    "norm_revenue_valid = norm_revenue_valid/global_max\n",
    "norm_revenue_test = norm_revenue_test/global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split xgboost data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost data \n",
    "xgb_trainx = processed_data[\"xgboost_data\"][\"train_x\"]\n",
    "xgb_trainy = processed_data[\"xgboost_data\"][\"train_y\"]\n",
    "xgb_validx = processed_data[\"xgboost_data\"][\"valid_x\"]\n",
    "xgb_validy = processed_data[\"xgboost_data\"][\"valid_y\"]\n",
    "xgb_testx = processed_data[\"xgboost_data\"][\"test_x\"]\n",
    "xgb_testy = processed_data[\"xgboost_data\"][\"test_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Define functions to be used\n",
    "### 2.3.1. \"find_best_threshod\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(model,x_list,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    The input arguments are;\n",
    "    - dtype model: scikit-learn classifier model\n",
    "    - dtype x_list: list or array of features (data)\n",
    "    - dtype y_test: array of true labels \n",
    "    '''\n",
    "    # Predict probability of fraud of each import.\n",
    "    y_pred_prob = model.predict_proba(x_list)[:,1]\n",
    "    # Set threshold range as [0.1, 0.2, ..., 0.5]. \n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    # Set an initial value of best threshold.\n",
    "    best_f1 = 0\n",
    "    # if best_thresh is set as \"None\", this function is to find the best_thresh as well as best_f1 \n",
    "    if best_thresh ==None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1\n",
    "    # if best_thresh is set as a certain number, this function is to calculate its f1 score.\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "    print(\"F1-scre equals to:%.4f\"%(best_f1))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Deploy a XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xgboost model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training xgboost model...\")\n",
    "columns = column_to_use\n",
    "xgb_trainx = pd.DataFrame(xgb_trainx,columns=columns)\n",
    "xgb_validx = pd.DataFrame(xgb_validx,columns=columns)\n",
    "xgb_testx = pd.DataFrame(xgb_testx,columns=columns)\n",
    "# Initiate the model\n",
    "xgb_clf = XGBClassifier(n_estimators=100, max_depth=4,n_jobs=-1)\n",
    "# Train/fit the model\n",
    "xgb_clf.fit(xgb_trainx,xgb_trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first tree out of 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"3478pt\" height=\"392pt\"\n",
       " viewBox=\"0.00 0.00 3478.03 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-388 3474.0317,-388 3474.0317,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1833.0911\" cy=\"-366\" rx=\"162.4712\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1833.0911\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.office.id&amp;importer.id&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1319.0911\" cy=\"-279\" rx=\"129.9769\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1319.0911\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.HS6&amp;country&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1743.8527,-350.8954C1650.933,-335.1678 1506.0514,-310.645 1411.8459,-294.6997\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1412.1988,-291.2098 1401.7549,-292.9917 1411.0305,-298.1116 1412.1988,-291.2098\"/>\n",
       "<text text-anchor=\"middle\" x=\"1651.5911\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2143.0911\" cy=\"-279\" rx=\"146.774\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2143.0911\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tariff.code&lt;8.51698483e+09</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1892.8622,-349.2255C1944.7882,-334.6528 2019.9943,-313.5465 2074.3885,-298.2811\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2075.6103,-301.5735 2084.2926,-295.5015 2073.7188,-294.8338 2075.6103,-301.5735\"/>\n",
       "<text text-anchor=\"middle\" x=\"2016.0911\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"651.0911\" cy=\"-192\" rx=\"131.077\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"651.0911\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.office.id&amp;HS6&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1224.0924,-266.6274C1100.4694,-250.5268 885.1126,-222.4788 756.5525,-205.7352\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"756.6706,-202.2211 746.3023,-204.4003 755.7665,-209.1625 756.6706,-202.2211\"/>\n",
       "<text text-anchor=\"middle\" x=\"1069.5911\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1319.0911\" cy=\"-192\" rx=\"146.774\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1319.0911\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tariff.code&lt;8.70322176e+09</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1319.0911,-260.9735C1319.0911,-249.1918 1319.0911,-233.5607 1319.0911,-220.1581\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1322.5912,-220.0033 1319.0911,-210.0034 1315.5912,-220.0034 1322.5912,-220.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"1328.0911\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2143.0911\" cy=\"-192\" rx=\"113.9803\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2143.0911\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gross.weight&lt;178808</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;5 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>2&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2143.0911,-260.9735C2143.0911,-249.1918 2143.0911,-233.5607 2143.0911,-220.1581\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2146.5912,-220.0033 2143.0911,-210.0034 2139.5912,-220.0034 2146.5912,-220.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"2187.5911\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2790.0911\" cy=\"-192\" rx=\"103.9815\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2790.0911\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.quantity&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2242.0608,-265.6919C2366.0134,-249.0244 2576.7474,-220.6876 2697.5886,-204.4385\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2698.1186,-207.8988 2707.5629,-203.0973 2697.1857,-200.9613 2698.1186,-207.8988\"/>\n",
       "<text text-anchor=\"middle\" x=\"2515.0911\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"318.0911\" cy=\"-105\" rx=\"116.1796\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.importer.id&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M590.0138,-176.0429C532.6537,-161.0569 446.9359,-138.6621 387.1214,-123.0349\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"387.9419,-119.6319 377.3819,-120.4904 386.1724,-126.4046 387.9419,-119.6319\"/>\n",
       "<text text-anchor=\"middle\" x=\"549.5911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"651.0911\" cy=\"-105\" rx=\"116.1796\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"651.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.importer.id&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M651.0911,-173.9735C651.0911,-162.1918 651.0911,-146.5607 651.0911,-133.1581\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"654.5912,-133.0033 651.0911,-123.0034 647.5912,-133.0034 654.5912,-133.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"660.0911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1155.0911\" cy=\"-105\" rx=\"122.3786\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1155.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gross.weight&lt;192917.5</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1285.9033,-174.3943C1260.2557,-160.7885 1224.5914,-141.8691 1196.9788,-127.2209\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1198.4235,-124.0254 1187.9494,-122.4309 1195.1431,-130.2091 1198.4235,-124.0254\"/>\n",
       "<text text-anchor=\"middle\" x=\"1291.5911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1412.0911\" cy=\"-105\" rx=\"116.1796\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1412.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.importer.id&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;10 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1338.3608,-173.9735C1351.9527,-161.2586 1370.337,-144.0603 1385.3475,-130.0183\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1387.9345,-132.3909 1392.8461,-123.0034 1383.1523,-127.279 1387.9345,-132.3909\"/>\n",
       "<text text-anchor=\"middle\" x=\"1381.0911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"100.0911\" cy=\"-18\" rx=\"100.1823\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.191158012</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;15 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M274.0309,-88.1316C258.5359,-82.1498 241.019,-75.3315 225.0911,-69 200.2206,-59.1137 172.6673,-47.9001 149.7727,-38.5102\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"150.8473,-35.1679 140.2673,-34.6068 148.1881,-41.6432 150.8473,-35.1679\"/>\n",
       "<text text-anchor=\"middle\" x=\"269.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"318.0911\" cy=\"-18\" rx=\"100.1823\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.125358865</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;16 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M318.0911,-86.9735C318.0911,-75.1918 318.0911,-59.5607 318.0911,-46.1581\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"321.5912,-46.0033 318.0911,-36.0034 314.5912,-46.0034 321.5912,-46.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"327.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"524.0911\" cy=\"-18\" rx=\"88.2844\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.0984127</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;17 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M623.2769,-87.5188C614.2238,-81.7229 604.1711,-75.1737 595.0911,-69 582.265,-60.2793 568.3647,-50.3937 556.2917,-41.6632\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"558.0132,-38.5878 547.8652,-35.5432 553.8997,-44.2516 558.0132,-38.5878\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>18</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"733.0911\" cy=\"-18\" rx=\"102.8821\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"733.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.0841121525</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;18 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M668.0816,-86.9735C679.9559,-74.3752 695.9786,-57.3755 709.1457,-43.4055\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"711.8105,-45.6811 716.1224,-36.0034 706.7165,-40.8799 711.8105,-45.6811\"/>\n",
       "<text text-anchor=\"middle\" x=\"706.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>19</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"951.0911\" cy=\"-18\" rx=\"96.6831\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"951.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.173071533</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;19 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>9&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1114.4347,-87.8224C1100.3826,-81.8743 1084.5398,-75.1558 1070.0911,-69 1046.6743,-59.0233 1020.689,-47.8936 998.9897,-38.5831\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1000.23,-35.3067 989.6602,-34.5789 997.4692,-41.7393 1000.23,-35.3067\"/>\n",
       "<text text-anchor=\"middle\" x=\"1114.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>20</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1172.0911\" cy=\"-18\" rx=\"105.8812\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1172.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.0181818195</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;20 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>9&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1158.6135,-86.9735C1160.9157,-75.1918 1163.9701,-59.5607 1166.589,-46.1581\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1170.0904,-46.489 1168.5732,-36.0034 1163.2204,-45.1465 1170.0904,-46.489\"/>\n",
       "<text text-anchor=\"middle\" x=\"1174.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>21</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1396.0911\" cy=\"-18\" rx=\"100.1823\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1396.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.109897614</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;21 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>10&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1408.7759,-86.9735C1406.6091,-75.1918 1403.7345,-59.5607 1401.2696,-46.1581\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1404.6532,-45.2053 1399.4021,-36.0034 1397.7686,-46.4715 1404.6532,-45.2053\"/>\n",
       "<text text-anchor=\"middle\" x=\"1449.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>22</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1617.0911\" cy=\"-18\" rx=\"102.8821\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1617.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.0857142881</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;22 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>10&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1452.8057,-88.0395C1467.1318,-82.0497 1483.3361,-75.2499 1498.0911,-69 1521.4387,-59.1105 1547.3166,-48.0338 1568.9614,-38.7364\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1570.4652,-41.8998 1578.2705,-34.7354 1567.7011,-35.4686 1570.4652,-41.8998\"/>\n",
       "<text text-anchor=\"middle\" x=\"1540.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2053.0911\" cy=\"-105\" rx=\"81.4863\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2053.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">quantity&lt;8227</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2124.4431,-173.9735C2111.2019,-161.1738 2093.2609,-143.8308 2078.682,-129.7379\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2081.0299,-127.1395 2071.4074,-122.7057 2076.1647,-132.1724 2081.0299,-127.1395\"/>\n",
       "<text text-anchor=\"middle\" x=\"2148.5911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2280.0911\" cy=\"-105\" rx=\"103.9815\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2280.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.quantity&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;12 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>5&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2170.8151,-174.3943C2191.7986,-161.069 2220.8083,-142.6468 2243.6647,-128.1321\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2245.8429,-130.895 2252.4084,-122.5796 2242.0904,-124.9858 2245.8429,-130.895\"/>\n",
       "<text text-anchor=\"middle\" x=\"2229.0911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2790.0911\" cy=\"-105\" rx=\"131.077\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2790.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.office.id&amp;HS6&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2790.0911,-173.9735C2790.0911,-162.1918 2790.0911,-146.5607 2790.0911,-133.1581\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2793.5912,-133.0033 2790.0911,-123.0034 2786.5912,-133.0034 2793.5912,-133.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"2834.5911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"3143.0911\" cy=\"-105\" rx=\"122.3786\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"3143.0911\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gross.weight&lt;277220.5</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;14 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>6&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2849.9728,-177.2416C2910.8736,-162.2321 3005.3033,-138.959 3070.4428,-122.9048\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"3071.3626,-126.2829 3080.2345,-120.4916 3069.6875,-119.4863 3071.3626,-126.2829\"/>\n",
       "<text text-anchor=\"middle\" x=\"2997.0911\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>23</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1835.0911\" cy=\"-18\" rx=\"96.6831\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1835.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.185764134</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;23 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>11&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2012.2861,-89.3874C1996.021,-83.1165 1977.1452,-75.7792 1960.0911,-69 1935.1253,-59.0758 1907.4561,-47.8142 1884.5097,-38.4024\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1885.5663,-35.0527 1874.9863,-34.4913 1882.907,-41.5279 1885.5663,-35.0527\"/>\n",
       "<text text-anchor=\"middle\" x=\"2004.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2053.0911\" cy=\"-18\" rx=\"102.8821\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2053.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.0571428612</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>11&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2053.0911,-86.9735C2053.0911,-75.1918 2053.0911,-59.5607 2053.0911,-46.1581\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2056.5912,-46.0033 2053.0911,-36.0034 2049.5912,-46.0034 2056.5912,-46.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"2062.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2280.0911\" cy=\"-18\" rx=\"105.8812\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2280.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.0068965517</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>12&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2280.0911,-86.9735C2280.0911,-75.1918 2280.0911,-59.5607 2280.0911,-46.1581\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2283.5912,-46.0033 2280.0911,-36.0034 2276.5912,-46.0034 2283.5912,-46.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"2324.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>26</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2495.0911\" cy=\"-18\" rx=\"90.9839\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2495.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.15384616</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;26 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>12&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2323.2454,-88.5696C2338.9716,-82.5098 2356.8693,-75.529 2373.0911,-69 2397.6886,-59.1 2424.9245,-47.7518 2447.4112,-38.2774\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2448.8842,-41.4548 2456.736,-34.3414 2446.162,-35.0057 2448.8842,-41.4548\"/>\n",
       "<text text-anchor=\"middle\" x=\"2416.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2710.0911\" cy=\"-18\" rx=\"105.8812\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2710.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.0425963514</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>13&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2773.5151,-86.9735C2761.9304,-74.3752 2746.2985,-57.3755 2733.4525,-43.4055\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2735.9911,-40.9953 2726.6459,-36.0034 2730.8383,-45.7335 2735.9911,-40.9953\"/>\n",
       "<text text-anchor=\"middle\" x=\"2799.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2931.0911\" cy=\"-18\" rx=\"96.6831\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2931.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.186363637</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>13&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2818.6246,-87.3943C2840.459,-73.922 2870.7372,-55.2397 2894.3774,-40.6531\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2896.4088,-43.5124 2903.0813,-35.2827 2892.733,-37.5551 2896.4088,-43.5124\"/>\n",
       "<text text-anchor=\"middle\" x=\"2878.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"3143.0911\" cy=\"-18\" rx=\"96.6831\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"3143.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.160000011</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>14&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M3143.0911,-86.9735C3143.0911,-75.1918 3143.0911,-59.5607 3143.0911,-46.1581\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"3146.5912,-46.0033 3143.0911,-36.0034 3139.5912,-46.0034 3146.5912,-46.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"3187.5911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"3364.0911\" cy=\"-18\" rx=\"105.8812\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"3364.0911\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.0500000007</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>14&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M3187.1221,-88.0576C3202.6145,-82.0694 3220.1373,-75.2659 3236.0911,-69 3261.5311,-59.0083 3289.7601,-47.7763 3313.2209,-38.4015\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"3314.641,-41.6032 3322.6267,-34.6405 3312.042,-35.1035 3314.641,-41.6032\"/>\n",
       "<text text-anchor=\"middle\" x=\"3280.0911\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x2b4d97a51350>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb.to_graphviz(booster = xgb_clf, num_trees=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Evaluating xgboost model------\n",
      "F1-scre equals to:0.2054\n",
      "AUC = 0.7498, F1-score = 0.2054\n"
     ]
    }
   ],
   "source": [
    "# evaluate xgboost model\n",
    "print(\"------Evaluating xgboost model------\")\n",
    "# Predict\n",
    "test_pred = xgb_clf.predict_proba(xgb_testx)[:,1]\n",
    "# Calculate auc\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "# Find the best threshold\n",
    "xgb_threshold,_ = find_best_threshold(xgb_clf, xgb_validx, xgb_validy)\n",
    "# Calculate the best f1 score\n",
    "xgb_f1 = find_best_threshold(xgb_clf, xgb_testx, xgb_testy,best_thresh=xgb_threshold)\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. DATE model (XGB + Neural Networks + Attention) <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Set environmenst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranger import Ranger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Preprocessing data: Integer-encoding of IMPORTER.TIN and TARIFF.CODE for attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user & item information \n",
    "train_raw_importers = train['importer.id'].values\n",
    "train_raw_items = train['tariff.code'].values\n",
    "valid_raw_importers = valid['importer.id'].values\n",
    "valid_raw_items = valid['tariff.code'].values\n",
    "test_raw_importers = test['importer.id']\n",
    "test_raw_items = test['tariff.code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need padding for unseen user or item \n",
    "importer_set = set(train_raw_importers)\n",
    "item_set = set(train_raw_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to +1 for zero padding \n",
    "importer_mapping = {v:i+1 for i,v in enumerate(importer_set)} \n",
    "hs6_mapping = {v:i+1 for i,v in enumerate(item_set)}\n",
    "importer_size = len(importer_mapping) + 1\n",
    "item_size = len(hs6_mapping) + 1\n",
    "# label-encoding\n",
    "train_importers = [importer_mapping[x] for x in train_raw_importers]\n",
    "train_items = [hs6_mapping[x] for x in train_raw_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data, we use padding_idx=0 for unseen data\n",
    "# use dic.get(key,deafault) to handle unseen\n",
    "valid_importers = [importer_mapping.get(x,0) for x in valid_raw_importers]\n",
    "valid_items = [hs6_mapping.get(x,0) for x in valid_raw_items]\n",
    "test_importers = [importer_mapping.get(x,0) for x in test_raw_importers] \n",
    "test_items = [hs6_mapping.get(x,0) for x in test_raw_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. \"process_leaf_idx\" function [(link to practice)](#practice5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_leaf_idx(X_leaves): \n",
    "    '''\n",
    "    This function is to convert the output of XGBoost model to the input of DATE model.\n",
    "    For an individual import, the output of XGBoost model is a list of leaf index of multiple trees.\n",
    "    eg. [1, 1, 10, 9, 30, 30, 32, ... ]\n",
    "    How to distinguish \"node 1\" of the first tree from \"node 1\" of the second tree?\n",
    "    How to distinguish \"node 30\" of the fifth tree from \"node 30\" of the sixth tree?\n",
    "    This function is to assign unique index to every leaf node in all the trees. \n",
    "    This function returns;\n",
    "    - lists of unique leaf index;\n",
    "    - total number of unique leaf nodes; and\n",
    "    - a reference table (dictionary) composed of \"unique leaf index\", \"tree id\", \"(previous) leaf index\". \n",
    "    '''\n",
    "    leaves = X_leaves.copy()\n",
    "    new_leaf_index = dict() # dictionary to store leaf index\n",
    "    total_leaves = 0\n",
    "    for c in range(X_leaves.shape[1]): # iterate for each column (ie. 100 trees)\n",
    "        column = X_leaves[:,c]\n",
    "        unique_vals = list(sorted(set(column)))\n",
    "        new_idx = {v:(i+total_leaves) for i,v in enumerate(unique_vals)}\n",
    "        for i,v in enumerate(unique_vals):\n",
    "            leaf_id = i+total_leaves\n",
    "            new_leaf_index[leaf_id] = {c:v}\n",
    "        leaves[:,c] = [new_idx[v] for v in column]\n",
    "        total_leaves += len(unique_vals)\n",
    "        \n",
    "    assert leaves.ravel().max() == total_leaves - 1\n",
    "    return leaves,total_leaves,new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. \"fgsm_attack\" function (Not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, loss, images, labels, eps) :\n",
    "    # images.requires_grad = True\n",
    "    images = Variable(images, requires_grad=True)\n",
    "    outputs = model.module.pred_from_hidden(images)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    cost = loss(outputs, labels)\n",
    "    cost.backward()\n",
    "    attack_images = images + eps * images.grad.sign()\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. \"metrics\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_prob,xgb_testy,revenue_test,best_thresh=None):\n",
    "    if best_thresh ==None:\n",
    "        _,overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "    else:\n",
    "        overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "    \n",
    "    pr, re, f, rev = [], [], [], []\n",
    "    for i in [99,98,95,90]:\n",
    "        threshold = np.percentile(y_prob, i)\n",
    "        #print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "        precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "        recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "\n",
    "        # save results\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "        f.append(f1)\n",
    "        rev.append(revenue_recall)\n",
    "        # print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')\n",
    "    return overall_f1,auc,pr, re, f, rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. \"torch_threshold\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_threshold(y_pred_prob,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    '''\n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    best_f1 = 0\n",
    "    if best_thresh == None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1, roc_auc_score(y_test, y_pred_prob)\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "        return best_f1, roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Identify leaf nodes of individual import from XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "train_rows = train.shape[0]\n",
    "valid_rows = valid.shape[0] + train_rows\n",
    "X_leaves = np.concatenate((X_train_leaves, X_valid_leaves, X_test_leaves), axis=0) # make sure the dimensionality\n",
    "transformed_leaves, leaf_num, new_leaf_index = process_leaf_idx(X_leaves)\n",
    "train_leaves, valid_leaves, test_leaves = transformed_leaves[:train_rows],\\\n",
    "                                          transformed_leaves[train_rows:valid_rows],\\\n",
    "                                          transformed_leaves[valid_rows:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Convert data to tensor\n",
    "Tensor is a collection of numbers with specific shape (dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch type\n",
    "train_leaves = torch.tensor(train_leaves).long()\n",
    "train_user = torch.tensor(train_importers).long()\n",
    "train_item = torch.tensor(train_items).long()\n",
    "\n",
    "valid_leaves = torch.tensor(valid_leaves).long()\n",
    "valid_user = torch.tensor(valid_importers).long()\n",
    "valid_item = torch.tensor(valid_items).long()\n",
    "\n",
    "test_leaves = torch.tensor(test_leaves).long()\n",
    "test_user = torch.tensor(test_importers).long()\n",
    "test_item = torch.tensor(test_items).long()\n",
    "\n",
    "# cls data\n",
    "train_label_cls = torch.tensor(xgb_trainy).float()\n",
    "valid_label_cls = torch.tensor(xgb_validy).float()\n",
    "test_label_cls = torch.tensor(xgb_testy).float()\n",
    "\n",
    "# revenue data \n",
    "train_label_reg = torch.tensor(norm_revenue_train).float()\n",
    "valid_label_reg = torch.tensor(revenue_valid).float()\n",
    "test_label_reg = torch.tensor(revenue_test).float()\n",
    "\n",
    "# create dataloader \n",
    "\n",
    "train_dataset = Data.TensorDataset(train_leaves,train_user,train_item,train_label_cls,train_label_reg)\n",
    "valid_dataset = Data.TensorDataset(valid_leaves,valid_user,valid_item,valid_label_cls,valid_label_reg)\n",
    "test_dataset = Data.TensorDataset(test_leaves,test_user,test_item,test_label_cls,test_label_reg)\n",
    "\n",
    "\n",
    "\n",
    "data4embedding = {\"train_dataset\":train_dataset,\"valid_dataset\":valid_dataset,\"test_dataset\":test_dataset,\\\n",
    "                  \"leaf_num\":leaf_num,\"importer_num\":importer_size,\"item_size\":item_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Save and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"torch_data.pickle\", 'wb') as f:\n",
    "    pickle.dump(data4embedding, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"leaf_index.pickle\", \"wb\") as f:\n",
    "    pickle.dump(new_leaf_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load torch dataset \n",
    "with open(\"torch_data.pickle\",\"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get torch dataset \n",
    "train_dataset = data[\"train_dataset\"]\n",
    "valid_dataset = data[\"valid_dataset\"]\n",
    "test_dataset = data[\"test_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=True,               \n",
    ")\n",
    "valid_loader = Data.DataLoader(\n",
    "    dataset=valid_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "\n",
    "# parameters for model \n",
    "leaf_num = data[\"leaf_num\"]\n",
    "importer_size = data[\"importer_num\"]\n",
    "item_size = data[\"item_size\"]\n",
    "\n",
    "# global variables\n",
    "xgb_validy = valid_loader.dataset.tensors[-2].detach().numpy()\n",
    "xgb_testy = test_loader.dataset.tensors[-2].detach().numpy()\n",
    "revenue_valid = valid_loader.dataset.tensors[-1].detach().numpy()\n",
    "revenue_test = test_loader.dataset.tensors[-1].detach().numpy()\n",
    "\n",
    "# model information\n",
    "curr_time = str(time.time())\n",
    "model_name = \"DATE\"\n",
    "model_path = \"./saved_models/%s%s.pkl\" % (model_name,curr_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import numpy as np \n",
    "from torch_multi_head_attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Mish()](#practice6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mish,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x *( torch.tanh(F.softplus(x))) #softplus???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice FusionAttention()](#practice7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim):\n",
    "        super(FusionAttention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, dim) # nn.Linear(size of input, size of output)\n",
    "        self.project_weight = nn.Linear(dim,1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        query_project = self.attention_matrix(inputs) # (b,t,d) -> (b,t,d2)\n",
    "        query_project = F.leaky_relu(query_project)\n",
    "        project_value = self.project_weight(query_project) # (b,t,h) -> (b,t,1)\n",
    "        attention_weight = torch.softmax(project_value, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = inputs * attention_weight\n",
    "        attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Attention()](#practice8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim,hidden,aggregate=\"sum\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, hidden)\n",
    "        self.project_weight = nn.Linear(hidden*2,hidden)\n",
    "        self.h = nn.Parameter(torch.rand(hidden,1))\n",
    "        self.agg_type = aggregate\n",
    "        \n",
    "    def forward(self, query, key): # query: 256 X 16, # key: 256 X 100 X 16, # assume key==value\n",
    "        dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "        batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "        time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "        \n",
    "        # concate input query and key \n",
    "        query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "        query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "        cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "        \n",
    "        # project to single value\n",
    "        project_vector = self.project_weight(cat_vector) \n",
    "        project_vector = torch.relu(project_vector)\n",
    "        attention_alpha = torch.matmul(project_vector,self.h)\n",
    "        attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = key * attention_weight\n",
    "        \n",
    "        # aggregate leaves\n",
    "        if self.agg_type == \"max\":\n",
    "            attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"mean\":\n",
    "            attention_vec = torch.mean(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"sum\":\n",
    "            attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice DATE()](#practice9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_leaf,importer_size,item_size,dim,\n",
    "                 head_num=4,fusion_type=\"concat\",act=\"relu\",device=\"cpu\",use_self=True,agg_type=\"sum\"):\n",
    "        super(DATE, self).__init__()\n",
    "        self.d = dim\n",
    "        self.device = device\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.LeakyReLU()\n",
    "        elif act == \"mish\":\n",
    "            self.act = Mish() \n",
    "        self.fusion_type = fusion_type\n",
    "        self.use_self = use_self\n",
    "\n",
    "        # embedding layers \n",
    "        self.leaf_embedding = nn.Embedding(max_leaf,dim)\n",
    "        self.user_embedding = nn.Embedding(importer_size,dim,padding_idx=0)\n",
    "        self.user_embedding.weight.data[0] = torch.zeros(dim) # unseen data? initial value?\n",
    "        self.item_embedding = nn.Embedding(item_size,dim,padding_idx=0)\n",
    "        self.item_embedding.weight.data[0] = torch.zeros(dim)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_bolck = Attention(dim,dim,agg_type).to(device)\n",
    "        self.self_att = MultiHeadAttention(dim,head_num).to(device)\n",
    "        self.fusion_att = FusionAttention(dim)\n",
    "\n",
    "        # Hidden & output layer\n",
    "        self.layer_norm = nn.LayerNorm((100,dim))\n",
    "        self.fussionlayer = nn.Linear(dim*3,dim)\n",
    "        self.hidden = nn.Linear(dim,dim)\n",
    "        self.output_cls_layer = nn.Linear(dim,1)\n",
    "        self.output_reg_layer = nn.Linear(dim,1)\n",
    "    \n",
    "    def forward(self,feature,uid,item_id):\n",
    "        \n",
    "        # Embedding of leaf_id\n",
    "        leaf_vectors = self.leaf_embedding(feature)\n",
    "        \n",
    "        # 1st attention: Multi-Head Self-Attention\n",
    "        # Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "        if self.use_self:\n",
    "            # Apply multy head attention\n",
    "            leaf_vectors = self.self_att(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "            # Normalization?\n",
    "            leaf_vectors = self.layer_norm(leaf_vectors)\n",
    "        \n",
    "        # Embedding of importer_id\n",
    "        importer_vector = self.user_embedding(uid)\n",
    "        # Embedding of item_id\n",
    "        item_vector = self.item_embedding(item_id)\n",
    "        # Multiply embeddings of importer_id and item_id\n",
    "        query_vector = importer_vector * item_vector\n",
    "        \n",
    "        # 2nd attention: Attention with leaf_id, importer_id and item_id (all embeddings)\n",
    "        set_vector, self.attention_w = self.attention_bolck(query_vector,leaf_vectors)\n",
    "        \n",
    "        # concat the user, item and tree vectors into a fusion attention\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=-1) # attach as columns\n",
    "            fusion = self.act(self.fussionlayer(fusion))\n",
    "        elif self.fusion_type == \"attention\":\n",
    "            importer_vector=importer_vector.view(-1,1,self.d), \n",
    "            item_vector=item_vector.view(-1,1,self.d), \n",
    "            set_vector=set_vector.view(-1,1,self.d)\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "            fusion,_ = self.fusion_att(fusion)\n",
    "        else:\n",
    "            raise \"Fusion type error\"\n",
    "        hidden = self.hidden(fusion)\n",
    "        hidden = self.act(hidden)\n",
    "\n",
    "        # multi-task output \n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        regression_output = torch.relu(self.output_reg_layer(hidden))\n",
    "        return classification_output, regression_output, hidden\n",
    "\n",
    "    def pred_from_hidden(self,hidden):\n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        return classification_output \n",
    "\n",
    "    def eval_on_batch(self,test_loader): # predict test data using batch \n",
    "        final_output = []\n",
    "        cls_loss = []\n",
    "        reg_loss = []\n",
    "        for batch in test_loader:\n",
    "            batch_feature, batch_user, batch_item, batch_cls, batch_reg = batch\n",
    "            batch_feature,batch_user,batch_item,batch_cls,batch_reg =  \\\n",
    "            batch_feature.to(self.device), batch_user.to(self.device),\\\n",
    "            batch_item.to(self.device), batch_cls.to(self.device), batch_reg.to(self.device)\n",
    "            batch_cls,batch_reg = batch_cls.view(-1,1), batch_reg.view(-1,1)\n",
    "            y_pred_prob, y_pred_rev,_ = self.forward(batch_feature,batch_user,batch_item)\n",
    "\n",
    "            # compute classification loss\n",
    "            cls_losses = nn.BCELoss()(y_pred_prob,batch_cls)\n",
    "            cls_loss.append(cls_losses.item())\n",
    "\n",
    "            # compute regression loss \n",
    "            reg_losses = nn.MSELoss()(y_pred_rev, batch_reg)\n",
    "            reg_loss.append(reg_losses.item())\n",
    "\n",
    "            # store predicted probability \n",
    "            y_pred = y_pred_prob.detach().cpu().numpy().tolist()\n",
    "            final_output.extend(y_pred)\n",
    "\n",
    "        print(\"CLS loss: %.4f, REG loss: %.4f\"% (np.mean(cls_loss), np.mean(reg_loss)) )\n",
    "        return np.array(final_output).ravel(), np.mean(cls_loss)+ np.mean(reg_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Train (hyperparameter + loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.1. Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--save'], dest='save', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help='save model or not', metavar=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', \n",
    "                        type=str, \n",
    "                        default=\"DATE\", \n",
    "                        help=\"Name of model\",\n",
    "                        )\n",
    "parser.add_argument('--epoch', \n",
    "                        type=int, \n",
    "                        default=5, \n",
    "                        help=\"Number of epochs\",\n",
    "                        )\n",
    "parser.add_argument('--dim', \n",
    "                        type=int, \n",
    "                        default=16, \n",
    "                        help=\"Hidden layer dimension\",\n",
    "                        )\n",
    "parser.add_argument('--lr', \n",
    "                        type=float, \n",
    "                        default=0.005, \n",
    "                        help=\"learning rate\",\n",
    "                        )\n",
    "parser.add_argument('--l2',\n",
    "                        type=float,\n",
    "                        default=0.00,\n",
    "                        help=\"l2 reg\",\n",
    "                        )\n",
    "parser.add_argument('--alpha',\n",
    "                        type=float,\n",
    "                        default=10,\n",
    "                        help=\"Regression loss weight\",\n",
    "                        )\n",
    "parser.add_argument('--beta', type=float, default=0.00, help=\"Adversarial loss weight\")\n",
    "parser.add_argument('--head_num', type=int, default=4, help=\"Number of heads for self attention\")\n",
    "parser.add_argument('--use_self', type=int, default=1, help=\"Wheter to use self attention\")\n",
    "parser.add_argument('--fusion', type=str, choices=[\"concat\",\"attention\"], default=\"concat\", help=\"Fusion method for final embedding\")\n",
    "parser.add_argument('--agg', type=str, choices=[\"sum\",\"max\",\"mean\"], default=\"sum\", help=\"Aggreate type for leaf embedding\")\n",
    "parser.add_argument('--act', type=str, choices=[\"mish\",\"relu\"], default=\"relu\", help=\"Activation function\")\n",
    "parser.add_argument('--device', type=str, choices=[\"cuda:0\",\"cuda:1\",\"cpu\"], default=\"cuda:0\", help=\"device name for training\")\n",
    "parser.add_argument('--output', type=str, default=\"full.csv\", help=\"Name of output file\")\n",
    "parser.add_argument('--save', type=int, default=0, help=\"save model or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "# Refer to: https://stackoverflow.com/questions/50763033/argparse-in-jupyter-notebook-throws-a-typeerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # get configs\n",
    "    epochs = 20 #args.epoch\n",
    "    dim = args.dim\n",
    "    lr = args.lr\n",
    "    weight_decay = args.l2\n",
    "    head_num = args.head_num\n",
    "    device = 'cuda:0'  #'cpu' if you run this code on cpu    #args.device\n",
    "    act = args.act\n",
    "    fusion = args.fusion\n",
    "    beta = args.beta\n",
    "    alpha = args.alpha\n",
    "    use_self = args.use_self\n",
    "    agg = args.agg\n",
    "    model = DATE(leaf_num,importer_size,item_size,\\\n",
    "                                    dim,head_num,\\\n",
    "                                    fusion_type=fusion,act=act,device=device,\\\n",
    "                                    use_self=use_self,agg_type=agg,\n",
    "                                    ).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=[0,1])\n",
    "\n",
    "    # initialize parameters\n",
    "    # Fills the input Tensor with values according to the method described in \n",
    "    # Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), \n",
    "    # using a uniform distribution.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # optimizer & loss \n",
    "    optimizer = Ranger(model.parameters(), weight_decay=weight_decay,lr=lr)\n",
    "    cls_loss_func = nn.BCELoss()\n",
    "    reg_loss_func = nn.MSELoss()\n",
    "\n",
    "    # save best model\n",
    "    global_best_score = 0\n",
    "    model_state = None\n",
    "\n",
    "    # early stop settings \n",
    "    stop_rounds = 3\n",
    "    no_improvement = 0\n",
    "    current_score = None \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, (batch_feature,batch_user,batch_item,batch_cls,batch_reg) in enumerate(train_loader):\n",
    "            model.train() # prep to train model\n",
    "            batch_feature,batch_user,batch_item,batch_cls,batch_reg =  \\\n",
    "            batch_feature.to(device), batch_user.to(device), batch_item.to(device),\\\n",
    "             batch_cls.to(device), batch_reg.to(device)\n",
    "            batch_cls,batch_reg = batch_cls.view(-1,1), batch_reg.view(-1,1)\n",
    "\n",
    "            # model output\n",
    "            classification_output, regression_output, hidden_vector = model(batch_feature,batch_user,batch_item)\n",
    "\n",
    "            # FGSM attack\n",
    "            adv_vector = fgsm_attack(model,cls_loss_func,hidden_vector,batch_cls,0.01)\n",
    "            adv_output = model.module.pred_from_hidden(adv_vector) \n",
    "\n",
    "            # calculate loss\n",
    "            adv_loss_func = nn.BCELoss(weight=batch_cls) \n",
    "            adv_loss = beta * adv_loss_func(adv_output,batch_cls) \n",
    "            cls_loss = cls_loss_func(classification_output,batch_cls)\n",
    "            revenue_loss = alpha * reg_loss_func(regression_output, batch_reg)\n",
    "            loss = cls_loss + revenue_loss + adv_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 1000 ==0:  \n",
    "                print(\"CLS loss:%.4f, REG loss:%.4f, ADV loss:%.4f, Loss:%.4f\"\\\n",
    "                %(cls_loss.item(),revenue_loss.item(),adv_loss.item(),loss.item()))\n",
    "                \n",
    "        # evaluate \n",
    "        model.eval()\n",
    "        print(\"Validate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(valid_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "        overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_validy,revenue_valid)\n",
    "        select_best = np.mean(f1s)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" % (overall_f1, auc, select_best) )\n",
    "\n",
    "        print(\"Evaluate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(test_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_testy,revenue_test,best_thresh=best_threshold)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" %(overall_f1, auc, np.mean(f1s)) )\n",
    "\n",
    "        # save best model \n",
    "        if select_best > global_best_score:\n",
    "            global_best_score = select_best\n",
    "            torch.save(model,model_path)\n",
    "        \n",
    "         # early stopping \n",
    "        if current_score == None:\n",
    "            current_score = select_best\n",
    "            continue\n",
    "        if select_best < current_score:\n",
    "            current_score = select_best\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= stop_rounds:\n",
    "            print(\"Early stopping...\")\n",
    "            break \n",
    "        if select_best > current_score:\n",
    "            no_improvement = 0\n",
    "            current_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate at epoch 1\n",
      "CLS loss: 0.2571, REG loss: 105252.8187\n",
      "Over-all F1:0.2153, AUC:0.7950, F1-top:0.1351\n",
      "Evaluate at epoch 1\n",
      "CLS loss: 0.2610, REG loss: 116774.4090\n",
      "Over-all F1:0.2146, AUC:0.7929, F1-top:0.1395\n",
      "Validate at epoch 2\n",
      "CLS loss: 0.2849, REG loss: 105255.6288\n",
      "Over-all F1:0.1656, AUC:0.7851, F1-top:0.1261\n",
      "Evaluate at epoch 2\n",
      "CLS loss: 0.2921, REG loss: 116777.0393\n",
      "Over-all F1:0.1719, AUC:0.7777, F1-top:0.1319\n",
      "Validate at epoch 3\n",
      "CLS loss: 0.2854, REG loss: 105251.8487\n",
      "Over-all F1:0.2053, AUC:0.7533, F1-top:0.1340\n",
      "Evaluate at epoch 3\n",
      "CLS loss: 0.2933, REG loss: 116773.7773\n",
      "Over-all F1:0.2044, AUC:0.7469, F1-top:0.1339\n",
      "Validate at epoch 4\n",
      "CLS loss: 0.3214, REG loss: 105246.4218\n",
      "Over-all F1:0.2114, AUC:0.7278, F1-top:0.1317\n",
      "Evaluate at epoch 4\n",
      "CLS loss: 0.3335, REG loss: 116768.9343\n",
      "Over-all F1:0.2115, AUC:0.7187, F1-top:0.1297\n",
      "Validate at epoch 5\n",
      "CLS loss: 0.3577, REG loss: 105257.8718\n",
      "Over-all F1:0.1085, AUC:0.7127, F1-top:0.1311\n",
      "Evaluate at epoch 5\n",
      "CLS loss: 0.3704, REG loss: 116780.5237\n",
      "Over-all F1:0.1051, AUC:0.7059, F1-top:0.1284\n",
      "Validate at epoch 6\n",
      "CLS loss: 0.3575, REG loss: 105252.7102\n",
      "Over-all F1:0.1655, AUC:0.7036, F1-top:0.1317\n",
      "Evaluate at epoch 6\n",
      "CLS loss: 0.3730, REG loss: 116776.3023\n",
      "Over-all F1:0.1483, AUC:0.6936, F1-top:0.1245\n",
      "Validate at epoch 7\n",
      "CLS loss: 0.3610, REG loss: 105253.6184\n",
      "Over-all F1:0.1515, AUC:0.6837, F1-top:0.1288\n",
      "Evaluate at epoch 7\n",
      "CLS loss: 0.3752, REG loss: 116777.2466\n",
      "Over-all F1:0.1358, AUC:0.6765, F1-top:0.1210\n",
      "Validate at epoch 8\n",
      "CLS loss: 0.3646, REG loss: 105253.5979\n",
      "Over-all F1:0.1535, AUC:0.6573, F1-top:0.1298\n",
      "Evaluate at epoch 8\n",
      "CLS loss: 0.3817, REG loss: 116777.7054\n",
      "Over-all F1:0.1455, AUC:0.6520, F1-top:0.1199\n",
      "Validate at epoch 9\n",
      "CLS loss: 0.4014, REG loss: 105254.3992\n",
      "Over-all F1:0.1306, AUC:0.6365, F1-top:0.1207\n",
      "Evaluate at epoch 9\n",
      "CLS loss: 0.4182, REG loss: 116777.4640\n",
      "Over-all F1:0.1268, AUC:0.6347, F1-top:0.1142\n",
      "Validate at epoch 10\n",
      "CLS loss: 0.4185, REG loss: 105247.8427\n",
      "Over-all F1:0.1828, AUC:0.6409, F1-top:0.1262\n",
      "Evaluate at epoch 10\n",
      "CLS loss: 0.4341, REG loss: 116771.2416\n",
      "Over-all F1:0.1684, AUC:0.6379, F1-top:0.1197\n",
      "Validate at epoch 11\n",
      "CLS loss: 0.4135, REG loss: 105252.9056\n",
      "Over-all F1:0.1540, AUC:0.6150, F1-top:0.1218\n",
      "Evaluate at epoch 11\n",
      "CLS loss: 0.4279, REG loss: 116775.8912\n",
      "Over-all F1:0.1443, AUC:0.6121, F1-top:0.1183\n",
      "Validate at epoch 12\n",
      "CLS loss: 0.4441, REG loss: 105253.4271\n",
      "Over-all F1:0.1353, AUC:0.5886, F1-top:0.1150\n",
      "Evaluate at epoch 12\n",
      "CLS loss: 0.4587, REG loss: 116775.8661\n",
      "Over-all F1:0.1395, AUC:0.5862, F1-top:0.1109\n",
      "Validate at epoch 13\n",
      "CLS loss: 0.4681, REG loss: 105254.9691\n",
      "Over-all F1:0.1220, AUC:0.5598, F1-top:0.1087\n",
      "Evaluate at epoch 13\n",
      "CLS loss: 0.4819, REG loss: 116776.9426\n",
      "Over-all F1:0.1257, AUC:0.5570, F1-top:0.1062\n",
      "Validate at epoch 14\n",
      "CLS loss: 0.4763, REG loss: 105254.4389\n",
      "Over-all F1:0.1341, AUC:0.5630, F1-top:0.1097\n",
      "Evaluate at epoch 14\n",
      "CLS loss: 0.4897, REG loss: 116776.5548\n",
      "Over-all F1:0.1333, AUC:0.5605, F1-top:0.1073\n",
      "Validate at epoch 15\n",
      "CLS loss: 0.5038, REG loss: 105254.5868\n",
      "Over-all F1:0.1289, AUC:0.5418, F1-top:0.1097\n",
      "Evaluate at epoch 15\n",
      "CLS loss: 0.5199, REG loss: 116777.0158\n",
      "Over-all F1:0.1343, AUC:0.5389, F1-top:0.1065\n",
      "Validate at epoch 16\n",
      "CLS loss: 0.5450, REG loss: 105251.9312\n",
      "Over-all F1:0.1384, AUC:0.5275, F1-top:0.1071\n",
      "Evaluate at epoch 16\n",
      "CLS loss: 0.5664, REG loss: 116774.5830\n",
      "Over-all F1:0.1433, AUC:0.5241, F1-top:0.1074\n",
      "Validate at epoch 17\n",
      "CLS loss: 0.5289, REG loss: 105254.8737\n",
      "Over-all F1:0.1250, AUC:0.5056, F1-top:0.1013\n",
      "Evaluate at epoch 17\n",
      "CLS loss: 0.5454, REG loss: 116776.8746\n",
      "Over-all F1:0.1311, AUC:0.5011, F1-top:0.1032\n",
      "Validate at epoch 18\n",
      "CLS loss: 0.5596, REG loss: 105255.0870\n",
      "Over-all F1:0.1235, AUC:0.5023, F1-top:0.0981\n",
      "Evaluate at epoch 18\n",
      "CLS loss: 0.5755, REG loss: 116776.6274\n",
      "Over-all F1:0.1326, AUC:0.4980, F1-top:0.1001\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Evaluation <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(save_model):\n",
    "    print()\n",
    "    print(\"--------Evaluating DATE model---------\")\n",
    "    # create best model\n",
    "    best_model = torch.load(model_path)\n",
    "    best_model.eval()\n",
    "\n",
    "    # get threshold\n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(valid_loader)\n",
    "    best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "\n",
    "    # predict test \n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(test_loader)\n",
    "    overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_testy,revenue_test,best_threshold)\n",
    "    best_score = f1s[0]\n",
    "    #os.system(\"rm %s\"%model_path)\n",
    "    if save_model:\n",
    "        scroed_name = \"./saved_models/%s_%.4f.pkl\" % (model_name,overall_f1)\n",
    "        torch.save(best_model,scroed_name)\n",
    "    \n",
    "    return overall_f1, auc, precisions, recalls, f1s, revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Evaluating DATE model---------\n",
      "CLS loss: 0.2571, REG loss: 105252.8187\n",
      "CLS loss: 0.2610, REG loss: 116774.4090\n"
     ]
    }
   ],
   "source": [
    "overall_f1, auc, precisions, recalls, f1s, revenues = evaluate(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving result... ./results/full.csv\n",
      "\n",
      "Metrics:\n",
      "f1:0.2146 auc:0.7929\n",
      "Pr@1:0.2350 Pr@2:0.2100 Pr@5:0.1920 Pr@10:0.2320\n",
      "Re@1:0.0309 Re@2:0.0552 Re@5:0.1262 Re@10:0.3051\n",
      "Rev@1:0.0333 Rev@2:0.0584 Rev@5:0.1268 Rev@10:0.3152\n"
     ]
    }
   ],
   "source": [
    "# save result\n",
    "output_file =  \"./results/full.csv\"\n",
    "print(\"Saving result...\",output_file)\n",
    "with open(output_file, 'a') as ff:\n",
    "    # print(args,file=ff)\n",
    "    print()\n",
    "    print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\" \\\n",
    "          % (overall_f1, auc,\\\n",
    "             precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "             recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "             revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "            ),\n",
    "        ) \n",
    "    output_metric = [16,overall_f1,auc] + precisions + recalls + revenues\n",
    "    output_metric = list(map(str,output_metric))\n",
    "    print(\" \".join(output_metric),file=ff)\n",
    "        \n",
    "    # print(\"Model:%s epoch:%d dim:%d lr:%f l2:%f beta:%f heads:%d fusion:%s activation:%s\"\n",
    "    #       % (model_name, epochs, dim, lr, weight_decay, beta, head_num,fusion,act),file=ff) \n",
    "    # print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\"  \\\n",
    "    #       % (overall_f1, auc,\\\n",
    "    #          precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "    #          recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "    #          revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "    #          ),\n",
    "    #          file=ff)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Practice of functions <a id='part5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_attributes <a id='practice1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sgd.date</th>\n",
       "      <th>office.id</th>\n",
       "      <th>importer.id</th>\n",
       "      <th>declarant.id</th>\n",
       "      <th>tariff.code</th>\n",
       "      <th>country</th>\n",
       "      <th>cif.value</th>\n",
       "      <th>quantity</th>\n",
       "      <th>gross.weight</th>\n",
       "      <th>total.taxes</th>\n",
       "      <th>revenue</th>\n",
       "      <th>illicit</th>\n",
       "      <th>tariff.code&amp;country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85303</th>\n",
       "      <td>13-11-18</td>\n",
       "      <td>OFFICE59</td>\n",
       "      <td>IMP648653</td>\n",
       "      <td>DEC5544</td>\n",
       "      <td>8703241128</td>\n",
       "      <td>CNTRY759</td>\n",
       "      <td>8335</td>\n",
       "      <td>4</td>\n",
       "      <td>14344</td>\n",
       "      <td>4188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8703241128CNTRY759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83064</th>\n",
       "      <td>13-11-08</td>\n",
       "      <td>OFFICE81</td>\n",
       "      <td>IMP617843</td>\n",
       "      <td>DEC9556</td>\n",
       "      <td>8703210011</td>\n",
       "      <td>CNTRY939</td>\n",
       "      <td>3398</td>\n",
       "      <td>63</td>\n",
       "      <td>92044</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8703210011CNTRY939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25070</th>\n",
       "      <td>13-04-19</td>\n",
       "      <td>OFFICE92</td>\n",
       "      <td>IMP606401</td>\n",
       "      <td>DEC1200</td>\n",
       "      <td>9505900000</td>\n",
       "      <td>CNTRY825</td>\n",
       "      <td>1477</td>\n",
       "      <td>3</td>\n",
       "      <td>852</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9505900000CNTRY825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sgd.date office.id importer.id declarant.id  tariff.code   country  \\\n",
       "85303  13-11-18  OFFICE59   IMP648653      DEC5544   8703241128  CNTRY759   \n",
       "83064  13-11-08  OFFICE81   IMP617843      DEC9556   8703210011  CNTRY939   \n",
       "25070  13-04-19  OFFICE92   IMP606401      DEC1200   9505900000  CNTRY825   \n",
       "\n",
       "       cif.value  quantity  gross.weight  total.taxes  revenue  illicit  \\\n",
       "85303       8335         4         14344         4188        0        0   \n",
       "83064       3398        63         92044          188        0        0   \n",
       "25070       1477         3           852          700        0        0   \n",
       "\n",
       "      tariff.code&country  \n",
       "85303  8703241128CNTRY759  \n",
       "83064  8703210011CNTRY939  \n",
       "25070  9505900000CNTRY825  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a small data for the demonstration.\n",
    "df_sample = df.sample(3)\n",
    "merge_attributes(df_sample, 'tariff.code','country')\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can identify that new column \"TARIFF.CODE&ORIGIN.CODE\" has been created.  \n",
    "You can learn each step of the function by replicating the codes line by line as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check what happens from \"iterables = [df[arg].astype(str) for arg in args]\"\n",
      "=====\n",
      "[88531    4203300000\n",
      "58655    8703241128\n",
      "7861     4115100000\n",
      "Name: tariff.code, dtype: object, 88531    CNTRY915\n",
      "58655    CNTRY759\n",
      "7861     CNTRY759\n",
      "Name: country, dtype: object]\n",
      "\n",
      "Check what happens from \"zip(*iterables)\"\n",
      "=====\n",
      "[('4203300000', 'CNTRY915'), ('8703241128', 'CNTRY759'), ('4115100000', 'CNTRY759')]\n",
      "\n",
      "What happens from \"fs = [.join([v for v in var]) for var in zip(*iterables)]\" - part1\n",
      "=====\n",
      "[('4203300000', 'CNTRY915'), ('8703241128', 'CNTRY759'), ('4115100000', 'CNTRY759')]\n",
      "\n",
      "What happens from \"fs = [.join([v for v in var]) for var in zip(*iterables)]\" - part2\n",
      "=====\n",
      "['4203300000CNTRY915', '8703241128CNTRY759', '4115100000CNTRY759']\n",
      "\n",
      "What happens from \"columnName = '&'.join([*args])'&'.join([*args])\"\n",
      "=====\n",
      "tariff.code&country\n",
      "\n",
      "df_sample[columnName]\n",
      "=====\n",
      "88531    4203300000CNTRY915\n",
      "58655    8703241128CNTRY759\n",
      "7861     4115100000CNTRY759\n",
      "Name: tariff.code&country, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.sample(3)\n",
    "print('Check what happens from \"iterables = [df[arg].astype(str) for arg in args]\"\\n=====')\n",
    "iterables = [df_sample[arg].astype(str) for arg in ['tariff.code','country']] \n",
    "print(iterables)\n",
    "print('')\n",
    "print('Check what happens from \"zip(*iterables)\"\\n=====')\n",
    "print(list(zip(*iterables)))\n",
    "print('')\n",
    "print('What happens from \"fs = [''.join([v for v in var]) for var in zip(*iterables)]\" - part1\\n=====')\n",
    "print(list(var for var in zip(*iterables)))\n",
    "print('')\n",
    "print('What happens from \"fs = [''.join([v for v in var]) for var in zip(*iterables)]\" - part2\\n=====')\n",
    "fs = [''.join([v for v in var]) for var in zip(*iterables)]\n",
    "print(fs)\n",
    "print('')\n",
    "print('What happens from \"columnName = \\'&\\'.join([*args])\\'&\\'.join([*args])\"\\n=====')\n",
    "columnName = '&'.join(['tariff.code','country'])\n",
    "print(columnName)\n",
    "print('')\n",
    "df_sample.loc[:, columnName] = fs\n",
    "print('df_sample[columnName]\\n=====')\n",
    "print(df_sample[columnName])\n",
    "\n",
    "# Now, we finish the line-by-line practice of the \"merge_attributes\" fuction, and delete temporarily created data. \n",
    "\n",
    "del df_sample, iterables, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess <a id='practice2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sgd.date</th>\n",
       "      <th>office.id</th>\n",
       "      <th>importer.id</th>\n",
       "      <th>declarant.id</th>\n",
       "      <th>tariff.code</th>\n",
       "      <th>country</th>\n",
       "      <th>cif.value</th>\n",
       "      <th>quantity</th>\n",
       "      <th>gross.weight</th>\n",
       "      <th>total.taxes</th>\n",
       "      <th>revenue</th>\n",
       "      <th>illicit</th>\n",
       "      <th>Unitprice</th>\n",
       "      <th>WUnitprice</th>\n",
       "      <th>TaxRatio</th>\n",
       "      <th>TaxUnitquantity</th>\n",
       "      <th>HS6</th>\n",
       "      <th>HS4</th>\n",
       "      <th>HS2</th>\n",
       "      <th>HS6&amp;country</th>\n",
       "      <th>office.id&amp;importer.id</th>\n",
       "      <th>office.id&amp;HS6</th>\n",
       "      <th>office.id&amp;country</th>\n",
       "      <th>SGD.DayofYear</th>\n",
       "      <th>SGD.WeekofYear</th>\n",
       "      <th>SGD.MonthofYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56241</th>\n",
       "      <td>13-08-14</td>\n",
       "      <td>OFFICE92</td>\n",
       "      <td>IMP894296</td>\n",
       "      <td>DEC7484</td>\n",
       "      <td>8714190000</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>92274</td>\n",
       "      <td>4</td>\n",
       "      <td>1694</td>\n",
       "      <td>8431</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23068.500000</td>\n",
       "      <td>54.471074</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>2107.750000</td>\n",
       "      <td>871419</td>\n",
       "      <td>8714</td>\n",
       "      <td>87</td>\n",
       "      <td>871419CNTRY680</td>\n",
       "      <td>OFFICE92IMP894296</td>\n",
       "      <td>OFFICE92871419</td>\n",
       "      <td>OFFICE92CNTRY680</td>\n",
       "      <td>225</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13932</th>\n",
       "      <td>13-03-06</td>\n",
       "      <td>OFFICE40</td>\n",
       "      <td>IMP759377</td>\n",
       "      <td>DEC8223</td>\n",
       "      <td>8703321922</td>\n",
       "      <td>CNTRY759</td>\n",
       "      <td>34342</td>\n",
       "      <td>17</td>\n",
       "      <td>415700</td>\n",
       "      <td>3661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020.117647</td>\n",
       "      <td>0.082612</td>\n",
       "      <td>0.106604</td>\n",
       "      <td>215.352941</td>\n",
       "      <td>870332</td>\n",
       "      <td>8703</td>\n",
       "      <td>87</td>\n",
       "      <td>870332CNTRY759</td>\n",
       "      <td>OFFICE40IMP759377</td>\n",
       "      <td>OFFICE40870332</td>\n",
       "      <td>OFFICE40CNTRY759</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79713</th>\n",
       "      <td>13-10-29</td>\n",
       "      <td>OFFICE40</td>\n",
       "      <td>IMP749280</td>\n",
       "      <td>DEC9870</td>\n",
       "      <td>2106901000</td>\n",
       "      <td>CNTRY653</td>\n",
       "      <td>3194</td>\n",
       "      <td>818</td>\n",
       "      <td>1008</td>\n",
       "      <td>458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.904645</td>\n",
       "      <td>3.168651</td>\n",
       "      <td>0.143394</td>\n",
       "      <td>0.559902</td>\n",
       "      <td>210690</td>\n",
       "      <td>2106</td>\n",
       "      <td>21</td>\n",
       "      <td>210690CNTRY653</td>\n",
       "      <td>OFFICE40IMP749280</td>\n",
       "      <td>OFFICE40210690</td>\n",
       "      <td>OFFICE40CNTRY653</td>\n",
       "      <td>301</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sgd.date office.id importer.id declarant.id  tariff.code   country  \\\n",
       "56241  13-08-14  OFFICE92   IMP894296      DEC7484   8714190000  CNTRY680   \n",
       "13932  13-03-06  OFFICE40   IMP759377      DEC8223   8703321922  CNTRY759   \n",
       "79713  13-10-29  OFFICE40   IMP749280      DEC9870   2106901000  CNTRY653   \n",
       "\n",
       "       cif.value  quantity  gross.weight  total.taxes  revenue  illicit  \\\n",
       "56241      92274         4          1694         8431        0        0   \n",
       "13932      34342        17        415700         3661        0        0   \n",
       "79713       3194       818          1008          458        0        0   \n",
       "\n",
       "          Unitprice  WUnitprice  TaxRatio  TaxUnitquantity     HS6   HS4  HS2  \\\n",
       "56241  23068.500000   54.471074  0.091369      2107.750000  871419  8714   87   \n",
       "13932   2020.117647    0.082612  0.106604       215.352941  870332  8703   87   \n",
       "79713      3.904645    3.168651  0.143394         0.559902  210690  2106   21   \n",
       "\n",
       "          HS6&country office.id&importer.id   office.id&HS6 office.id&country  \\\n",
       "56241  871419CNTRY680     OFFICE92IMP894296  OFFICE92871419  OFFICE92CNTRY680   \n",
       "13932  870332CNTRY759     OFFICE40IMP759377  OFFICE40870332  OFFICE40CNTRY759   \n",
       "79713  210690CNTRY653     OFFICE40IMP749280  OFFICE40210690  OFFICE40CNTRY653   \n",
       "\n",
       "       SGD.DayofYear  SGD.WeekofYear  SGD.MonthofYear  \n",
       "56241            225              32                7  \n",
       "13932             64               9                2  \n",
       "79713            301              43               10  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df.sample(3)\n",
    "df_sample = preprocess(df_sample)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_risk_profile <a id='practice3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMP243487',\n",
       " 'IMP264340',\n",
       " 'IMP692823',\n",
       " 'IMP913967',\n",
       " 'IMP505713',\n",
       " 'IMP865586',\n",
       " 'IMP355376',\n",
       " 'IMP358915',\n",
       " 'IMP361374',\n",
       " 'IMP366360']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df.sample(100)\n",
    "\n",
    "# We will identify top 10% of high risk importers in terms of its non-compliance records. \n",
    "find_risk_profile(df=df_sample, feature='importer.id', topk_ratio=0.1, adj=10, option='topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag_risky_profiles <a id='practice4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMP221442',\n",
       " 'IMP317314',\n",
       " 'IMP606401',\n",
       " 'IMP643025',\n",
       " 'IMP783306',\n",
       " 'IMP438805',\n",
       " 'IMP225876',\n",
       " 'IMP412513',\n",
       " 'IMP358915',\n",
       " 'IMP359656']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a sample data\n",
    "df_sample = df.sample(100)\n",
    "# Identify top 10% high-risk importer\n",
    "risk_profiles = find_risk_profile(df_sample, 'importer.id', 0.1, 10, option='topk')\n",
    "risk_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sgd.date</th>\n",
       "      <th>office.id</th>\n",
       "      <th>importer.id</th>\n",
       "      <th>declarant.id</th>\n",
       "      <th>tariff.code</th>\n",
       "      <th>country</th>\n",
       "      <th>cif.value</th>\n",
       "      <th>quantity</th>\n",
       "      <th>gross.weight</th>\n",
       "      <th>total.taxes</th>\n",
       "      <th>revenue</th>\n",
       "      <th>illicit</th>\n",
       "      <th>RiskH.importer.id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97391</th>\n",
       "      <td>13-12-23</td>\n",
       "      <td>OFFICE40</td>\n",
       "      <td>IMP606401</td>\n",
       "      <td>DEC3641</td>\n",
       "      <td>8516800000</td>\n",
       "      <td>CNTRY415</td>\n",
       "      <td>3577</td>\n",
       "      <td>1</td>\n",
       "      <td>565</td>\n",
       "      <td>510</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38968</th>\n",
       "      <td>13-06-11</td>\n",
       "      <td>OFFICE51</td>\n",
       "      <td>IMP359656</td>\n",
       "      <td>DEC8254</td>\n",
       "      <td>8703321922</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>8124</td>\n",
       "      <td>1</td>\n",
       "      <td>25077</td>\n",
       "      <td>4141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93071</th>\n",
       "      <td>13-12-09</td>\n",
       "      <td>OFFICE59</td>\n",
       "      <td>IMP358915</td>\n",
       "      <td>DEC6350</td>\n",
       "      <td>8703321922</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>4365</td>\n",
       "      <td>4</td>\n",
       "      <td>1993</td>\n",
       "      <td>4381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31041</th>\n",
       "      <td>13-05-13</td>\n",
       "      <td>OFFICE76</td>\n",
       "      <td>IMP412513</td>\n",
       "      <td>DEC3474</td>\n",
       "      <td>8704322000</td>\n",
       "      <td>CNTRY454</td>\n",
       "      <td>6729</td>\n",
       "      <td>1</td>\n",
       "      <td>8557</td>\n",
       "      <td>3222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5849</th>\n",
       "      <td>13-01-29</td>\n",
       "      <td>OFFICE60</td>\n",
       "      <td>IMP317314</td>\n",
       "      <td>DEC8824</td>\n",
       "      <td>8210000000</td>\n",
       "      <td>CNTRY859</td>\n",
       "      <td>4051</td>\n",
       "      <td>596</td>\n",
       "      <td>206579</td>\n",
       "      <td>4163</td>\n",
       "      <td>336</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>13-02-18</td>\n",
       "      <td>OFFICE76</td>\n",
       "      <td>IMP322276</td>\n",
       "      <td>DEC3949</td>\n",
       "      <td>8703332100</td>\n",
       "      <td>CNTRY615</td>\n",
       "      <td>4616</td>\n",
       "      <td>1342</td>\n",
       "      <td>236012</td>\n",
       "      <td>2690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70057</th>\n",
       "      <td>13-09-30</td>\n",
       "      <td>OFFICE60</td>\n",
       "      <td>IMP891508</td>\n",
       "      <td>DEC1704</td>\n",
       "      <td>9022120000</td>\n",
       "      <td>CNTRY874</td>\n",
       "      <td>1841</td>\n",
       "      <td>1</td>\n",
       "      <td>8582</td>\n",
       "      <td>726</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49383</th>\n",
       "      <td>13-07-17</td>\n",
       "      <td>OFFICE59</td>\n",
       "      <td>IMP839288</td>\n",
       "      <td>DEC5426</td>\n",
       "      <td>8703241128</td>\n",
       "      <td>CNTRY680</td>\n",
       "      <td>271655</td>\n",
       "      <td>1</td>\n",
       "      <td>376516</td>\n",
       "      <td>62042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92026</th>\n",
       "      <td>13-12-05</td>\n",
       "      <td>OFFICE76</td>\n",
       "      <td>IMP992026</td>\n",
       "      <td>DEC2777</td>\n",
       "      <td>8517180000</td>\n",
       "      <td>CNTRY759</td>\n",
       "      <td>9305</td>\n",
       "      <td>1</td>\n",
       "      <td>504647</td>\n",
       "      <td>3772</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93351</th>\n",
       "      <td>13-12-10</td>\n",
       "      <td>OFFICE51</td>\n",
       "      <td>IMP896567</td>\n",
       "      <td>DEC5070</td>\n",
       "      <td>7004900000</td>\n",
       "      <td>CNTRY316</td>\n",
       "      <td>1089058</td>\n",
       "      <td>84</td>\n",
       "      <td>991</td>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sgd.date office.id importer.id declarant.id  tariff.code   country  \\\n",
       "97391  13-12-23  OFFICE40   IMP606401      DEC3641   8516800000  CNTRY415   \n",
       "38968  13-06-11  OFFICE51   IMP359656      DEC8254   8703321922  CNTRY680   \n",
       "93071  13-12-09  OFFICE59   IMP358915      DEC6350   8703321922  CNTRY680   \n",
       "31041  13-05-13  OFFICE76   IMP412513      DEC3474   8704322000  CNTRY454   \n",
       "5849   13-01-29  OFFICE60   IMP317314      DEC8824   8210000000  CNTRY859   \n",
       "...         ...       ...         ...          ...          ...       ...   \n",
       "10500  13-02-18  OFFICE76   IMP322276      DEC3949   8703332100  CNTRY615   \n",
       "70057  13-09-30  OFFICE60   IMP891508      DEC1704   9022120000  CNTRY874   \n",
       "49383  13-07-17  OFFICE59   IMP839288      DEC5426   8703241128  CNTRY680   \n",
       "92026  13-12-05  OFFICE76   IMP992026      DEC2777   8517180000  CNTRY759   \n",
       "93351  13-12-10  OFFICE51   IMP896567      DEC5070   7004900000  CNTRY316   \n",
       "\n",
       "       cif.value  quantity  gross.weight  total.taxes  revenue  illicit  \\\n",
       "97391       3577         1           565          510     1442        1   \n",
       "38968       8124         1         25077         4141        0        0   \n",
       "93071       4365         4          1993         4381        0        0   \n",
       "31041       6729         1          8557         3222        0        0   \n",
       "5849        4051       596        206579         4163      336        1   \n",
       "...          ...       ...           ...          ...      ...      ...   \n",
       "10500       4616      1342        236012         2690        0        0   \n",
       "70057       1841         1          8582          726        0        0   \n",
       "49383     271655         1        376516        62042        0        0   \n",
       "92026       9305         1        504647         3772        0        0   \n",
       "93351    1089058        84           991          317        0        0   \n",
       "\n",
       "       RiskH.importer.id  \n",
       "97391                  1  \n",
       "38968                  1  \n",
       "93071                  1  \n",
       "31041                  1  \n",
       "5849                   1  \n",
       "...                  ...  \n",
       "10500                  0  \n",
       "70057                  0  \n",
       "49383                  0  \n",
       "92026                  0  \n",
       "93351                  0  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag \"RiskH\" to the top 10% high-risk importers\n",
    "aaa = tag_risky_profiles(df=df_sample, profile='importer.id', profiles=risk_profiles, option='topk')\n",
    "aaa.sort_values('RiskH.importer.id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample, aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_leaf_idx <a id='practice5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 13, 15, 15, 13, 13, 13,\n",
       "        13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 23, 15, 23, 17, 15,\n",
       "        17, 15, 15, 19, 15, 25, 15, 15, 15, 20, 17, 15, 16, 28, 15, 15,\n",
       "        17, 16, 16, 15, 17, 15, 21, 20, 15, 15, 20, 15, 17, 21, 15, 19,\n",
       "        16, 25, 16, 16, 18, 15, 15, 19, 22, 17, 17, 19, 17, 15, 16, 18,\n",
       "        19, 18, 19, 16, 16, 18, 18, 29, 15, 16, 21, 20, 17, 18, 16, 18,\n",
       "        16, 23, 15, 18],\n",
       "       [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 13, 15, 15, 13, 13, 13,\n",
       "        13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 23, 15, 23, 17, 15,\n",
       "        17, 15, 15, 19, 15, 25, 15, 15, 15, 20, 17, 15, 16, 28, 15, 15,\n",
       "        17, 16, 16, 15, 17, 15, 21, 20, 15, 15, 20, 15, 17, 21, 15, 20,\n",
       "        16, 15, 16, 16, 18, 15, 15, 19, 22, 15, 18, 20, 17, 15, 16, 19,\n",
       "        19, 18, 15, 20, 15, 18, 19, 29, 15, 19, 18, 23, 15, 21, 13, 18,\n",
       "        23, 23, 15, 18],\n",
       "       [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 13, 15, 15, 13, 13, 13,\n",
       "        13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 23, 15, 23, 17, 15,\n",
       "        17, 15, 15, 19, 15, 25, 15, 15, 15, 20, 17, 15, 16, 28, 15, 15,\n",
       "        17, 16, 16, 15, 17, 15, 21, 20, 15, 15, 20, 15, 17, 21, 15, 20,\n",
       "        16, 27, 16, 16, 18, 15, 15, 19, 22, 18, 18, 20, 17, 15, 16, 18,\n",
       "        19, 18, 22, 20, 15, 18, 18, 29, 15, 20, 27, 20, 15, 18, 16, 18,\n",
       "        23, 23, 15, 18]], dtype=int32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = X_train_leaves[:3] # 100 trees in our xgboost model!\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves, total_leaves, new_leaf_index = process_leaf_idx(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  65,\n",
       "         67,  69,  70,  71,  72,  73,  74,  75,  77,  79,  81,  83,  84,\n",
       "         85,  86,  88,  89,  91,  93,  96,  97,  98, 100, 101, 102, 106,\n",
       "        108, 111, 112, 115, 116, 117, 119, 120, 121],\n",
       "       [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  64,  65,\n",
       "         66,  69,  70,  71,  72,  73,  74,  75,  76,  80,  82,  83,  84,\n",
       "         85,  87,  88,  89,  90,  94,  95,  97,  99, 100, 101, 103, 105,\n",
       "        109, 110, 113, 114, 116, 118, 119, 120, 121],\n",
       "       [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  64,  65,\n",
       "         68,  69,  70,  71,  72,  73,  74,  75,  78,  80,  82,  83,  84,\n",
       "         85,  86,  88,  89,  92,  94,  95,  97,  98, 100, 101, 104, 107,\n",
       "        108, 110, 112, 115, 116, 118, 119, 120, 121]], dtype=int32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new idex of leaf\n",
    "leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 15},\n",
       " 1: {1: 15},\n",
       " 2: {2: 15},\n",
       " 3: {3: 15},\n",
       " 4: {4: 15},\n",
       " 5: {5: 15},\n",
       " 6: {6: 15},\n",
       " 7: {7: 15},\n",
       " 8: {8: 15},\n",
       " 9: {9: 15},\n",
       " 10: {10: 13},\n",
       " 11: {11: 15},\n",
       " 12: {12: 15},\n",
       " 13: {13: 13},\n",
       " 14: {14: 13},\n",
       " 15: {15: 13},\n",
       " 16: {16: 13},\n",
       " 17: {17: 15},\n",
       " 18: {18: 15},\n",
       " 19: {19: 15},\n",
       " 20: {20: 15},\n",
       " 21: {21: 15},\n",
       " 22: {22: 15},\n",
       " 23: {23: 15},\n",
       " 24: {24: 15},\n",
       " 25: {25: 15},\n",
       " 26: {26: 17},\n",
       " 27: {27: 23},\n",
       " 28: {28: 15},\n",
       " 29: {29: 23},\n",
       " 30: {30: 17},\n",
       " 31: {31: 15},\n",
       " 32: {32: 17},\n",
       " 33: {33: 15},\n",
       " 34: {34: 15},\n",
       " 35: {35: 19},\n",
       " 36: {36: 15},\n",
       " 37: {37: 25},\n",
       " 38: {38: 15},\n",
       " 39: {39: 15},\n",
       " 40: {40: 15},\n",
       " 41: {41: 20},\n",
       " 42: {42: 17},\n",
       " 43: {43: 15},\n",
       " 44: {44: 16},\n",
       " 45: {45: 28},\n",
       " 46: {46: 15},\n",
       " 47: {47: 15},\n",
       " 48: {48: 17},\n",
       " 49: {49: 16},\n",
       " 50: {50: 16},\n",
       " 51: {51: 15},\n",
       " 52: {52: 17},\n",
       " 53: {53: 15},\n",
       " 54: {54: 21},\n",
       " 55: {55: 20},\n",
       " 56: {56: 15},\n",
       " 57: {57: 15},\n",
       " 58: {58: 20},\n",
       " 59: {59: 15},\n",
       " 60: {60: 17},\n",
       " 61: {61: 21},\n",
       " 62: {62: 15},\n",
       " 63: {63: 19},\n",
       " 64: {63: 20},\n",
       " 65: {64: 16},\n",
       " 66: {65: 15},\n",
       " 67: {65: 25},\n",
       " 68: {65: 27},\n",
       " 69: {66: 16},\n",
       " 70: {67: 16},\n",
       " 71: {68: 18},\n",
       " 72: {69: 15},\n",
       " 73: {70: 15},\n",
       " 74: {71: 19},\n",
       " 75: {72: 22},\n",
       " 76: {73: 15},\n",
       " 77: {73: 17},\n",
       " 78: {73: 18},\n",
       " 79: {74: 17},\n",
       " 80: {74: 18},\n",
       " 81: {75: 19},\n",
       " 82: {75: 20},\n",
       " 83: {76: 17},\n",
       " 84: {77: 15},\n",
       " 85: {78: 16},\n",
       " 86: {79: 18},\n",
       " 87: {79: 19},\n",
       " 88: {80: 19},\n",
       " 89: {81: 18},\n",
       " 90: {82: 15},\n",
       " 91: {82: 19},\n",
       " 92: {82: 22},\n",
       " 93: {83: 16},\n",
       " 94: {83: 20},\n",
       " 95: {84: 15},\n",
       " 96: {84: 16},\n",
       " 97: {85: 18},\n",
       " 98: {86: 18},\n",
       " 99: {86: 19},\n",
       " 100: {87: 29},\n",
       " 101: {88: 15},\n",
       " 102: {89: 16},\n",
       " 103: {89: 19},\n",
       " 104: {89: 20},\n",
       " 105: {90: 18},\n",
       " 106: {90: 21},\n",
       " 107: {90: 27},\n",
       " 108: {91: 20},\n",
       " 109: {91: 23},\n",
       " 110: {92: 15},\n",
       " 111: {92: 17},\n",
       " 112: {93: 18},\n",
       " 113: {93: 21},\n",
       " 114: {94: 13},\n",
       " 115: {94: 16},\n",
       " 116: {95: 18},\n",
       " 117: {96: 16},\n",
       " 118: {96: 23},\n",
       " 119: {97: 23},\n",
       " 120: {98: 15},\n",
       " 121: {99: 18}}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: {new_leaf_index: {tree_index: leaf_index}}\n",
    "new_leaf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample, leaves, total_leaves, new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish <a id='practice6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[0.5517, 0.7971, 0.1953]]) Mish(x):  tensor([[0.4217, 0.6568, 0.1292]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1,3))\n",
    "Mish_test = Mish()\n",
    "print(\"x: \", x,\"Mish(x): \", Mish_test(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FusionAttention <a id='practice7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Set an arbitrary imput data\n",
      "=====\n",
      "torch.Size([256, 3, 16])\n",
      "\n",
      "1. query_project \n",
      "==============\n",
      "torch.Size([256, 3, 16])\n",
      "\n",
      "2. query_project \n",
      "==============\n",
      "torch.Size([256, 3, 16])\n",
      "\n",
      "3. project_value \n",
      "==============\n",
      "torch.Size([256, 3, 1])\n",
      "\n",
      "4. attention_weight \n",
      "==============\n",
      "torch.Size([256, 3, 1])\n",
      "\n",
      "5. attention_vec \n",
      "==============\n",
      "torch.Size([256, 3, 16])\n",
      "\n",
      "6. attention_vec \n",
      "==============\n",
      "torch.Size([256, 16])\n"
     ]
    }
   ],
   "source": [
    "print('0. Set an arbitrary imput data\\n=====')\n",
    "inputs_test = torch.randn(256,3,16)\n",
    "print(inputs_test.shape)\n",
    "dim_test=inputs_test.shape[-1]\n",
    "print('')\n",
    "print(\"1. query_project \\n==============\")\n",
    "query_project = nn.Linear(dim_test,dim_test)(inputs_test)\n",
    "print(query_project.shape)\n",
    "print('')\n",
    "print(\"2. query_project \\n==============\")\n",
    "query_project = F.leaky_relu(query_project)\n",
    "print(query_project.shape)\n",
    "print('')\n",
    "print(\"3. project_value \\n==============\")\n",
    "project_value = nn.Linear(dim_test,1)(query_project)\n",
    "print(project_value.shape)\n",
    "print('')\n",
    "print(\"4. attention_weight \\n==============\")\n",
    "attention_weight = torch.softmax(project_value, dim=1)\n",
    "print(attention_weight.shape)\n",
    "print('')\n",
    "print(\"5. attention_vec \\n==============\")\n",
    "attention_vec = inputs_test*attention_weight\n",
    "print(attention_vec.shape)\n",
    "print('')\n",
    "print(\"6. attention_vec \\n==============\")\n",
    "attention_vec = torch.sum(attention_vec, dim=1)\n",
    "print(attention_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention <a id='practice8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Set arbitrary \"query\" and \"key\"\n",
      "=====\n",
      "query: torch.Size([256, 16])\n",
      "key: torch.Size([256, 100, 16])\n",
      "\n",
      "2. Transform \"quary\" to be merged with \"key\"\n",
      "=====\n",
      "query: torch.Size([256, 100, 16])\n",
      "cat_vector: torch.Size([256, 100, 32])\n",
      "\n",
      "3. Neural Network\n",
      "=====\n",
      "3-1. project_vector: torch.Size([256, 100, 16])\n",
      "3-2. project_vector: torch.Size([256, 100, 16])\n",
      "3-3. h: torch.Size([16, 1])\n",
      "3-4. attention_weight: torch.Size([256, 100, 1])\n",
      "3-5. attention_vec: torch.Size([256, 100, 16])\n",
      "\n",
      "4. Result\n",
      "=====\n",
      "4-1. max_attention_vec: torch.Size([256, 16])\n",
      "4-2. mean_attention_vec: torch.Size([256, 16])\n",
      "4-3. sum_attention_vec: torch.Size([256, 16])\n"
     ]
    }
   ],
   "source": [
    "print('1. Set arbitrary \"query\" and \"key\"\\n=====')\n",
    "# \"query\" is the embedding layer of importer_id + HScode_id in the final model.\n",
    "# \"key\" is the embedding layer of leaf_id of 100 decision trees in the final model.\n",
    "query = torch.randn(256, 16) # (batch_size, n_embedding_dim)\n",
    "print('query:',query.shape)\n",
    "key = torch.randn(256, 100, 16) # (batch_size, n_trees, n_embedding_dim)\n",
    "print('key:',key.shape)\n",
    "print('')\n",
    "print('2. Transform \"quary\" to be merged with \"key\"\\n=====')\n",
    "dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "print('query:', query.shape)\n",
    "cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "print('cat_vector:',cat_vector.shape)\n",
    "print('')\n",
    "print('3. Neural Network\\n=====')\n",
    "project_vector = nn.Linear(32,16)(cat_vector)\n",
    "print('3-1. project_vector:',project_vector.shape)\n",
    "project_vector = torch.relu(project_vector)\n",
    "print('3-2. project_vector:',project_vector.shape)\n",
    "h = nn.Parameter(torch.rand(16,1))\n",
    "print('3-3. h:',h.shape)\n",
    "attention_alpha = torch.matmul(project_vector,h)\n",
    "attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "print('3-4. attention_weight:',attention_weight.shape)\n",
    "attention_vec = key * attention_weight # assumed 'key'=='value'\n",
    "print('3-5. attention_vec:',attention_vec.shape)\n",
    "print('')\n",
    "print('4. Result\\n=====')\n",
    "max_attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "print('4-1. max_attention_vec:',max_attention_vec.shape)\n",
    "mean_attention_vec = torch.mean(attention_vec,dim=1)\n",
    "print('4-2. mean_attention_vec:',mean_attention_vec.shape)\n",
    "sum_attention_vec = torch.sum(attention_vec,dim=1)\n",
    "print('4-3. sum_attention_vec:',sum_attention_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATE model <a id='practice9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Hyperparameters\n",
      "=====\n",
      "dim: 16\n",
      "leaf_num: 1516\n",
      "device: cpu\n",
      "item_size: 1877\n",
      "importer_size: 7342\n",
      "\n",
      "2. Toy data (batch)\n",
      "=====\n",
      "one batch from 'train_loader'\n",
      "feature: torch.Size([256, 100])\n",
      "uid: torch.Size([256])\n",
      "item_id: torch.Size([256])\n",
      "\n",
      "3. Embedding of feature (cross-features, leaf_ids)\n",
      "=====\n",
      "leaf_vectors: torch.Size([256, 100, 16])\n",
      "\n",
      "4. Multi-Head Self-Attention (1st attention) \n",
      "=====\n",
      "torch.Size([256, 100, 16])\n",
      "torch.Size([256, 100, 16])\n",
      "\n",
      "5. Embedding of importer_id\n",
      "=====\n",
      "torch.Size([256, 16])\n",
      "\n",
      "6. Embedding of item_id(HScode)\n",
      "=====\n",
      "torch.Size([256, 16])\n",
      " \n",
      "7. Multiply embeddings of importer_id and item_id\n",
      "=====\n",
      "torch.Size([256, 16])\n",
      " \n",
      "8. Attention with leaf_id, importer_id and item_id (2nd attention)\n",
      "=====\n",
      "torch.Size([256, 16])\n",
      "\n",
      "9. Concat the user_id, item_id and set(attention)_vector to fusion\n",
      "=====\n",
      "importer_vector: torch.Size([256, 1, 16])\n",
      "item_vector: torch.Size([256, 1, 16])\n",
      "set_vector: torch.Size([256, 1, 16])\n",
      "fusion: torch.Size([256, 3, 16])\n",
      "\n",
      "10. Fusion Attention\n",
      "=====\n",
      "fusion: torch.Size([256, 16])\n",
      "\n",
      "11. Task-specific layers\n",
      "=====\n",
      "hidden: torch.Size([256, 16])\n",
      "\n",
      "12. output\n",
      "=====\n",
      "classification_output: torch.Size([256, 1])\n",
      "regression_output: torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "head_num = 4\n",
    "\n",
    "print('1. Hyperparameters\\n=====')\n",
    "print('dim:',dim)\n",
    "print('leaf_num:',leaf_num)\n",
    "print('device:',device)\n",
    "print('item_size:',item_size)\n",
    "print('importer_size:',importer_size)\n",
    "print('')\n",
    "print('2. Toy data (batch)\\n=====\\none batch from \\'train_loader\\'')\n",
    "feature,uid,item_id,cls,reg = next(iter(train_loader))\n",
    "print('feature:',feature.shape)\n",
    "print('uid:',uid.shape)\n",
    "print('item_id:',item_id.shape)\n",
    "# other option\n",
    "#feature = train_loader.dataset.tensors[0][:256]\n",
    "#uid = train_loader.dataset.tensors[1][:256]\n",
    "#item_id = train_loader.dataset.tensors[2][:256]\n",
    "print('')\n",
    "print('3. Embedding of feature (cross-features, leaf_ids)\\n=====')\n",
    "leaf_vectors = nn.Embedding(leaf_num,dim)(feature)\n",
    "print('leaf_vectors:',leaf_vectors.shape)\n",
    "print('')\n",
    "print('4. Multi-Head Self-Attention (1st attention) \\n=====')\n",
    "# Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "# Apply multy head attention\n",
    "multiheadattention = MultiHeadAttention(dim,head_num).to(device)\n",
    "leaf_vectors = multiheadattention(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "print(leaf_vectors.shape)\n",
    "leaf_vectors = nn.LayerNorm((100,dim))(leaf_vectors)\n",
    "print(leaf_vectors.shape)\n",
    "print('')\n",
    "print('5. Embedding of importer_id\\n=====')\n",
    "importer_vector = nn.Embedding(importer_size,dim,padding_idx=0)(uid)\n",
    "print(importer_vector.shape)\n",
    "print('')\n",
    "print('6. Embedding of item_id(HScode)\\n=====')\n",
    "item_vector = nn.Embedding(item_size,dim,padding_idx=0)(item_id)\n",
    "print(item_vector.shape)\n",
    "print(' ')\n",
    "print('7. Multiply embeddings of importer_id and item_id\\n=====')\n",
    "query_vector = importer_vector * item_vector\n",
    "print(query_vector.shape)\n",
    "print(' ')\n",
    "print('8. Attention with leaf_id, importer_id and item_id (2nd attention)\\n=====')\n",
    "attention = Attention(dim,dim,\"sum\").to(device)\n",
    "set_vector, _ = attention(query_vector,leaf_vectors)\n",
    "print(set_vector.shape)\n",
    "print('')\n",
    "print('9. Concat the user_id, item_id and set(attention)_vector to fusion\\n=====')\n",
    "importer_vector=importer_vector.view(-1,1,dim)\n",
    "print('importer_vector:',importer_vector.shape)\n",
    "item_vector=item_vector.view(-1,1,dim) \n",
    "print('item_vector:',item_vector.shape)\n",
    "set_vector=set_vector.view(-1,1,dim)\n",
    "print('set_vector:',set_vector.shape)\n",
    "fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "print('fusion:', fusion.shape)\n",
    "print('')\n",
    "print('10. Fusion Attention\\n=====')\n",
    "fusion_att = FusionAttention(dim)\n",
    "fusion,_ = fusion_att(fusion) # fusion attention\n",
    "print('fusion:',fusion.shape)\n",
    "print('')\n",
    "print('11. Task-specific layers\\n=====')\n",
    "hidden = nn.Linear(dim,dim)(fusion)\n",
    "hidden = nn.LeakyReLU()(hidden)\n",
    "print('hidden:',hidden.shape)\n",
    "print('')\n",
    "print('12. output\\n=====')\n",
    "classification_output = torch.sigmoid(nn.Linear(dim,1)(hidden))\n",
    "print('classification_output:',classification_output.shape)\n",
    "regression_output = torch.relu(nn.Linear(dim,1)(hidden))\n",
    "print('regression_output:',regression_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex: XGBoost and Logistic Regression <a id='part6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Performance of XGBoost model\n",
    "Performance indicators;\n",
    "- precision = (number of seizures)/(number of inspections) --> targeting accuracy\n",
    "- recall = (number of seizures)/(number of actual frauds)\n",
    "- revenue_recall = (revenue from seizures)/(revenue from actual frauds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 200\n",
      "Precision: 0.20499999821186066, Recall: 0.027, Seized Revenue (Recall): 0.0314\n",
      "Checking top 2% suspicious transactions: 400\n",
      "Precision: 0.20250000059604645, Recall: 0.0533, Seized Revenue (Recall): 0.0546\n",
      "Checking top 5% suspicious transactions: 1000\n",
      "Precision: 0.1889999955892563, Recall: 0.1243, Seized Revenue (Recall): 0.1255\n",
      "Checking top 10% suspicious transactions: 2000\n",
      "Precision: 0.19050000607967377, Recall: 0.2505, Seized Revenue (Recall): 0.2686\n"
     ]
    }
   ],
   "source": [
    "# Precision and Recall\n",
    "y_prob = test_pred\n",
    "for i in [99,98,95,90]:\n",
    "    # Find the ith value in ascending order.\n",
    "    threshold = np.percentile(y_prob, i)\n",
    "    print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "    precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "    recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "    revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "    print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the structured trees in txt file\n",
    "xgb_clf.get_booster().dump_model('xgb_model.txt', with_stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. XGBoost + Logistic Regression model <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1. Deploy Xgboost+LR model \n",
    "Summary\n",
    "    1. Apply the pre-trained XGBoost model to train-, valid- and test-data. \n",
    "    2. For an individual import, the XGBoost model returns a list of leaf index of the multiple trees. \n",
    "    3. One-hot-encode leaf node.\n",
    "    4. The encoded leaf index is fed as an input into the logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply the pre-trained XGBoost model to train-, valid- and test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost+LR model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For an individual import, the XGBoost model returns a list of leaf index of the multiple trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 13, 15, 15, 13, 13, 13, 13,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 23, 15, 23, 17, 15, 17, 15,\n",
       "       15, 19, 15, 25, 15, 15, 15, 20, 17, 15, 16, 28, 15, 15, 17, 16, 16,\n",
       "       15, 17, 15, 21, 20, 15, 15, 20, 15, 17, 21, 15, 19, 16, 25, 16, 16,\n",
       "       18, 15, 15, 19, 22, 17, 17, 19, 17, 15, 16, 18, 19, 18, 19, 16, 16,\n",
       "       18, 18, 29, 15, 16, 21, 20, 17, 18, 16, 18, 16, 23, 15, 18],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the 100 leaf nodes of the first import in the train-data\n",
    "X_train_leaves[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. One-hot-encode the leaf (Create unique set of 0s and 1s for individual leaf index).\n",
    "For instance, \n",
    "    - Leaf index 1: [100000...00]\n",
    "    - Leaf index 2: [010000...00]\n",
    "    - Leaf index 3: [001000...00]\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for leaf index\n",
    "xgbenc = OneHotEncoder(categories=\"auto\")\n",
    "lr_trainx = xgbenc.fit_transform(X_train_leaves)\n",
    "lr_validx = xgbenc.transform(X_valid_leaves)\n",
    "lr_testx = xgbenc.transform(X_test_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Input the encoded leaf index into the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic regression model...\n",
      "------Evaluating xgboost+LR model------\n",
      "F1-scre equals to:0.1607\n",
      "AUC = 0.6581, F1-score = 0.1607\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "print(\"Training Logistic regression model...\")\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(lr_trainx, xgb_trainy)\n",
    "test_pred = lr.predict_proba(lr_testx)[:,1]\n",
    "print(\"------Evaluating xgboost+LR model------\")\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "xgb_threshold,_ = find_best_threshold(lr, lr_validx, xgb_validy) # threshold was select from validation set\n",
    "xgb_f1 = find_best_threshold(lr, lr_testx, xgb_testy,best_thresh=xgb_threshold) # then applied on test set\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2. Performance of XGBoost + LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 200\n",
      "Precision: 0.20000000298023224, Recall: 0.0263, Seized Revenue (Recall): 0.0285\n",
      "Checking top 2% suspicious transactions: 400\n",
      "Precision: 0.1875, Recall: 0.0493, Seized Revenue (Recall): 0.0534\n",
      "Checking top 5% suspicious transactions: 1000\n",
      "Precision: 0.17900000512599945, Recall: 0.1177, Seized Revenue (Recall): 0.1233\n",
      "Checking top 10% suspicious transactions: 2000\n",
      "Precision: 0.16599999368190765, Recall: 0.2183, Seized Revenue (Recall): 0.2291\n"
     ]
    }
   ],
   "source": [
    "# Precision and Recall\n",
    "y_prob = test_pred\n",
    "for i in [99,98,95,90]:\n",
    "    threshold = np.percentile(y_prob, i)\n",
    "    print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "    precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "    recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "    revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "    print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
